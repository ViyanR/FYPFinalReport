@article{chahineRobustFlightNavigation2023,
  title = {Robust Flight Navigation Out of Distribution with Liquid Neural Networks},
  author = {Chahine, Makram and Hasani, Ramin and Kao, Patrick and Ray, Aaron and Shubert, Ryan and Lechner, Mathias and Amini, Alexander and Rus, Daniela},
  journal = {Science Robotics},
  volume = {8},
  number = {77},
  pages = {eadc8892},
  year = {2023},
  month = apr,
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/scirobotics.adc8892},
  url = {https://www.science.org/doi/10.1126/scirobotics.adc8892}
}

@article{gruenbacherGoTubeScalableStatistical2022,
  title = {GoTube: Scalable Statistical Verification of Continuous-Depth Models},
  author = {Gruenbacher, Sophie A. and Lechner, Mathias and Hasani, Ramin and Rus, Daniela and Henzinger, Thomas A. and Smolka, Scott A. and Grosu, Radu},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {6},
  pages = {6755--6764},
  year = {2022},
  month = jun,
  doi = {10.1609/aaai.v36i6.20631},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/20631}
}

@article{grunbacherVerificationNeuralODEs2021,
  title = {On the Verification of Neural ODEs with Stochastic Guarantees},
  author = {Grunbacher, Sophie and Hasani, Ramin and Lechner, Mathias and Cyranka, Jacek and Smolka, Scott A. and Grosu, Radu},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {13},
  pages = {11525--11535},
  year = {2021},
  month = may,
  doi = {10.1609/aaai.v35i13.17372},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17372}
}

@article{hasaniLiquidTimeconstantNetworks2021,
  title = {Liquid Time-Constant Networks},
  author = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {9},
  pages = {7657--7666},
  year = {2021},
  month = may,
  doi = {10.1609/aaai.v35i9.16936},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/16936}
}

@inproceedings{henriksenEfficientNeuralNetwork,
  title = {Efficient Neural Network Verification via Adaptive Refinement and Adversarial Search},
  author = {Henriksen, Patrick and Lomuscio, Alessio},
  booktitle = {Proceedings of the 24th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS)},
  year = {2024},
  publisher = {Springer},
  note = {To appear}
}

@misc{hessionLiquidNeuralNets2024,
  title = {Liquid Neural Nets (LNNs)},
  author = {Hession, Jake},
  year = {2024},
  month = may,
  howpublished = {\url{https://medium.com/@hessionj/liquid-neural-nets-lnns-32ce1bfb045a}},
  note = {Medium article accessed 2025-01-20}
}}

@inproceedings{koPOPQORNQuantifyingRobustness2019,
  title = {POPQORN: Quantifying Robustness of Recurrent Neural Networks},
  author = {Ko, Ching-Yun and Lyu, Zhaoyang and Weng, Lily and Daniel, Luca and Wong, Ngai and Lin, Dahua},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  pages = {3468--3477},
  year = {2019},
  publisher = {PMLR}
}

@article{neherTaylorModelBased2007,
  title = {On Taylor Model Based Integration of ODEs},
  author = {Neher, M. and Jackson, K. R. and Nedialkov, N. S.},
  journal = {SIAM Journal on Numerical Analysis},
  volume = {45},
  number = {1},
  pages = {236--262},
  year = {2007},
  publisher = {Society for Industrial and Applied Mathematics},
  doi = {10.1137/050638448}
}

@article{regoLyapunovbasedContinuoustimeNonlinear2022,
  title = {Lyapunov-Based Continuous-Time Nonlinear Control Using Deep Neural Network Applied to Underactuated Systems},
  author = {Rego, Rosana C. B. and de Ara{\'u}jo, F{\'a}bio M. U.},
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {107},
  pages = {104519},
  year = {2022},
  doi = {10.1016/j.engappai.2021.104519},
  publisher = {Elsevier}
}

@misc{tedxtalksLiquidNeuralNetworks2023,
  title = {Liquid Neural Networks | Ramin Hasani | TEDxMIT},
  author = {{TEDx Talks}},
  year = {2023},
  month = jan,
  howpublished = {\url{https://www.ted.com/talks/ramin_hasani_liquid_neural_networks}},
  note = {Accessed 2025-01-20}
}

@article{tranVerificationPiecewiseDeep2021,
  title = {Verification of Piecewise Deep Neural Networks: A Star Set Approach with Zonotope Pre-Filter},
  shorttitle = {Verification of Piecewise Deep Neural Networks},
  author = {Tran, Hoang-Dung and Pal, Neelanjana and Lopez, Diego Manzanas and Musau, Patrick and Yang, Xiaodong and Nguyen, Luan Viet and Xiang, Weiming and Bak, Stanley and Johnson, Taylor T.},
  year = {2021},
  month = aug,
  journal = {Formal Aspects of Computing},
  volume = {33},
  number = {4-5},
  pages = {519--545},
  issn = {0934-5043, 1433-299X},
  doi = {10.1007/s00165-021-00553-4},
  urldate = {2025-01-22},
  abstract = {Abstract             Verification has emerged as a means to provide formal guarantees on learning-based systems incorporating neural network before using them in safety-critical applications. This paper proposes a new verification approach for deep neural networks (DNNs) with piecewise linear activation functions using reachability analysis. The core of our approach is a collection of reachability algorithms using star sets (or shortly, stars), an effective symbolic representation of high-dimensional polytopes. The star-based reachability algorithms compute the output reachable sets of a network with a given input set before using them for verification. For a neural network with piecewise linear activation functions, our approach can construct both exact and over-approximate reachable sets of the neural network. To enhance the scalability of our approach, a star set is equipped with an outer-zonotope (a zonotope over-approximation of the star set) to quickly estimate the lower and upper bounds of an input set at a specific neuron to determine if splitting occurs at that neuron. This zonotope pre-filtering step reduces significantly the number of linear programming optimization problems that must be solved in the analysis, and leads to a reduction in computation time, which enhances the scalability of the star set approach. Our reachability algorithms are implemented in a software prototype called the neural network verification tool, and can be applied to problems analyzing the robustness of machine learning methods, such as safety and robustness verification of DNNs. Our experiments show that our approach can achieve runtimes twenty to 1400 times faster than Reluplex, a satisfiability modulo theory-based approach. Our star set approach is also less conservative than other recent zonotope and abstract domain approaches.},
  langid = {english},
  file = {/Users/viyanraj/Zotero/storage/XQN962ST/Tran et al. - 2021 - Verification of piecewise deep neural networks a star set approach with zonotope pre-filter.pdf}
}

@article{tranVerificationPiecewiseDeep2021,
  title = {Verification of Piecewise Deep Neural Networks: A Star Set Approach with Zonotope Pre-Filter},
  author = {Tran, Hoang-Dung and Pal, Neelanjana and Lopez, Diego Manzanas and Musau, Patrick and Yang, Xiaodong and Nguyen, Luan Viet and Xiang, Weiming and Bak, Stanley and Johnson, Taylor T.},
  journal = {Formal Aspects of Computing},
  volume = {33},
  number = {4--5},
  pages = {519--545},
  year = {2021},
  doi = {10.1007/s00165-021-00553-4},
  publisher = {Springer}
}



@misc{WhatRecurrentNeural2021,
  title = {What Is a Recurrent Neural Network (RNN)? | IBM},
  year = {2021},
  month = oct,
  howpublished = {\url{https://www.ibm.com/think/topics/recurrent-neural-networks}},
  note = {Accessed 2025-01-21}
}

@incollection{xueRNNBasedFrameworkMILP2023,
  title = {An RNN-Based Framework for the MILP Problem in Robustness Verification of Neural Networks},
  author = {Xue, Hao and Zeng, Xia and Lin, Wang and Yang, Zhengfeng and Peng, Chao and Zeng, Zhenbing},
  booktitle = {Computer Vision -- ACCV 2022},
  editor = {Wang, Lei and Gall, Juergen and Chin, Tat-Jun and Sato, Imari and Chellappa, Rama},
  publisher = {Springer Nature Switzerland},
  volume = {13841},
  pages = {571--586},
  year = {2023},
  doi = {10.1007/978-3-031-26319-4_34}
}

@misc{zeqiriEfficientCertifiedTraining2023,
  title = {Efficient Certified Training and Robustness Verification of Neural ODEs},
  author = {Zeqiri, Mustafa and M{\"u}ller, Mark Niklas and Fischer, Marc and Vechev, Martin},
  year = {2023},
  month = mar,
  archiveprefix = {arXiv},
  eprint = {2303.05246},
  doi = {10.48550/arXiv.2303.05246},
  url = {https://arxiv.org/abs/2303.05246}
}

@misc{zhangEfficientNeuralNetwork2018,
  title = {Efficient {{Neural Network Robustness Certification}} with {{General Activation Functions}}},
  author = {Zhang, Huan and Weng, Tsui-Wei and Chen, Pin-Yu and Hsieh, Cho-Jui and Daniel, Luca},
  year = {2018},
  month = nov,
  number = {arXiv:1811.00866},
  eprint = {1811.00866},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.00866},
  urldate = {2025-01-23},
  abstract = {Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/viyanraj/Zotero/storage/YF2Q6BS6/Zhang et al. - 2018 - Efficient Neural Network Robustness Certification with General Activation Functions.pdf;/Users/viyanraj/Zotero/storage/ECTDFARJ/1811.html}
}

@inproceedings{madry2018towards,
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2018},
  url = {https://openreview.net/forum?id=rJzIBfZAb}
}

@article{goodfellow2015explaining,
  title = {Explaining and Harnessing Adversarial Examples},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  journal = {arXiv preprint},
  volume = {arXiv:1412.6572},
  year = {2015},
  archiveprefix = {arXiv},
  eprint = {1412.6572},
  url = {https://arxiv.org/abs/1412.6572}
}

@article{szegedy2014intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={International Conference on Learning Representations (ICLR)},
  year={2014}
}

@article{dupont2019augmented,
  title={Augmented neural ODEs},
  author={Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{tramer2018ensemble,
  title={Ensemble adversarial training: Attacks and defenses},
  author={Tram{\`e}r, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{cohen2019certified,
  title={Certified adversarial robustness via randomized smoothing},
  author={Cohen, Jeremy M and Rosenfeld, Elan and Kolter, Zico},
  journal={International Conference on Machine Learning (ICML)},
  year={2019}
}

@inproceedings{gehr2018ai2,
  title = {AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation},
  author = {Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
  booktitle = {IEEE Symposium on Security and Privacy (SP)},
  pages = {3--18},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/SP.2018.00058}
}


@inproceedings{vincent2008extracting,
  title={Extracting and composing robust features with denoising autoencoders},
  author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1096--1103},
  year={2008},
  organization={ACM}
}

@article{finlay2020trainable,
  title={Trainable verification of neural networks},
  author={Finlay, Chris and Papamakarios, George},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11427--11438},
  year={2020}
}

@inproceedings{wang2018adversarial,
  title={Adversarial defense by restricting the hidden space of deep neural networks},
  author={Wang, Yisen and Ma, Xingjun and Bailey, James and Zisselman, Evgeny and Ye, Jinfeng and Liu, Bowen and Zhu, Huan},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={350--366},
  year={2018},
  organization={Springer}
}

@article{zhang2022towards,
  title={Towards certifying LSTM robustness to adversarial perturbations},
  author={Zhang, Lening and Zhang, Kaidi and Zhang, Kaidi and Weng, Tsui-Wei},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={17},
  pages={439--454},
  year={2022},
  publisher={IEEE}
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@inproceedings{moosavi2016deepfool,
  title={DeepFool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2574--2582},
  year={2016}
}

@article{uesato2018adversarial,
  title={Adversarial risk and the dangers of evaluating against weak attacks},
  author={Uesato, Jonathan and O'Donoghue, Brendan and van den Oord, Aaron and Kohli, Pushmeet},
  journal={International Conference on Machine Learning (ICML)},
  year={2018}
}

@article{hasani2021liquid,
  title = {Liquid Time-Constant Networks},
  author = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {9},
  pages = {7657--7666},
  year = {2021},
  doi = {10.1609/aaai.v35i9.16936},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/16936}
}

@inproceedings{cisse2017houdini,
  title={Houdini: Fooling deep structured prediction models},
  author={Cisse, Moustapha and Adi, Yossi and Neverova, Natalia and Keshet, Joseph},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6977--6987},
  year={2017}
}

@inproceedings{sun2018natural,
  title={Natural and adversarial error detection using invariant predictors},
  author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018}
}

@inproceedings{weng2018evaluating,
  title={Evaluating the robustness of neural networks: An extreme value theory approach},
  author={Weng, Tsui-Wei and Zhang, Huan and Chen, Pin-Yu and Yi, Jinfeng and Su, Dong and Gao, Yanzhi and Daniel, Luca and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{dong2020benchmarking,
  title={Benchmarking Robustness of Neural Networks on Time-Series Data},
  author={Dong, Xiang and Yao, Shoukang and Hu, Han and Wang, Yiran and Li, Shen and Zhang, Zhaoran and Wang, Tarek Abdelzaher and others},
  journal={arXiv preprint arXiv:2003.12298},
  year={2020}
}

@misc{ltctutorial2022,
  author       = {Michael Hermann and Guillaume Bellec},
  title        = {LTCtutorial: Liquid Time-Constant Neural Networks},
  year         = {2022},
  howpublished = {\url{https://github.com/KPEKEP/LTCtutorial}},
  note         = {GitHub repository, accessed: 2025-06-13}
}


@article{frank2024learning,
  title={Learning Neuron Dynamics: The Age of SPIKING 2.0},
  author={Frank, Eno and Fekri, Faramarz},
  journal={arXiv preprint arXiv:2407.20590},
  year={2024},
  url={https://arxiv.org/abs/2407.20590}
}

@inproceedings{liquidTimeConstant,
  author    = {Ramin Hasani and Mathias Lechner and Alexander Amini and Daniela Rus and Radu Grosu},
  title     = {Liquid Time-Constant Networks},
  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing (ICONIP)},
  series    = {Lecture Notes in Computer Science},
  volume    = {12533},
  pages     = {650--661},
  year      = {2020},
  publisher = {Springer},
  doi       = {10.1007/978-3-030-60029-7\_50}
}

@misc{autoLiRPA2022,
  author       = {Zhengyao Wang and Huan Zhang and Tsui-Wei Weng and Luca Daniel and Cho-Jui Hsieh},
  title        = {auto\_LiRPA: A Library for Certified Robustness in PyTorch},
  year         = {2022},
  howpublished = {\url{https://github.com/Verified-Intelligence/auto_LiRPA}},
  note         = {Accessed: 2025-06-13}
}

@inproceedings{henriksen2023auto,
  title = {Auto-LiRPA: An Automatic Framework for Certified Robustness Evaluation},
  author = {Henriksen, Patrick and Wu, Xiaoyu and Wang, Shiqi and Jin, Hongge and Zhang, Haotian and Huan, Xueqi and Lomuscio, Alessio and Wang, Xiao},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2023},
  publisher = {PMLR},
  url = {https://openreview.net/forum?id=7MQD5XAK7UX}
}

@inproceedings{vaswani2017attention,
  title     = {Attention Is All You Need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems (NeurIPS)},
  year      = {2017},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}
}