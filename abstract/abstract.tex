\chapter*{Abstract}

The increasing deployment of neural networks in safety-critical dynamic systems (such as autonomous vehicles) has highlighted the need for formal guarantees of robustness, which quantify the model's ability to maintain correct outputs under adversarial input perturbations. However, the verification of such properties becomes significantly more complex with Liquid Neural Networks, which are a novel class of time-continuous architectures that use ODEs to provide improved adaptability and robustness in sequential learning tasks. The project addresses this gap by conducting a comprehensive adversarial evaluation of LNNs, benchmarking their resilience on a 2D timeseries input against a range of adversarial attacks, which aim to perturb the input slightly and cause significant output errors, exposing vulnerabilities. A diverse set of adversarial attacks, ranging from gradient-based methods (like PGD and FGSM) to gradient-free time-manipulation methods, were employed to expose different failure modes and sensitivities across temporal models. In addition, a novel verification pipeline was implemented using interval bound propagation through the auto\_LiRPA framework, producing verifiable bounds on prediction intervals. This enabled formal robustness certification of the LNN model behavior under $l_\infty$ perturbations. Results demonstrate that, given comparable predictive performance to their discrete-time counterparts, LNNs' continuous internal dynamics yield greater inherent resistance to adversarial manipulation. Under attack, they demonstrated lower empirical degradation than the comparative models. These findings reinforce the value of continuous-time and liquid neuron dynamics for building robust machine learning systems and provide a reproducible benchmark for future verification research in this domain.