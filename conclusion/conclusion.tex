\chapter{Conclusion}

\hl{Around 4 pages}

This project has explored the robustness of Logic Neural Networks (LNNs) against adversarial attacks, comparing them with traditional neural networks. The evaluation focused on three main types of attacks: temporal, structured, and spatial, each designed to exploit different vulnerabilities in the models.
The results indicate that LNNs exhibit superior robustness in temporal and structured adversarial regimes, where the attacks leverage the temporal dependencies and structured relationships inherent in the data. In contrast, traditional neural networks were more susceptible to these types of attacks, highlighting their limitations in handling complex data structures.


In summary, this evaluation demonstrates that:
\begin{enumerate}
    \item Robustness is a multidimensional property — not all attacks exploit the same vulnerabilities.
    \item LNNs, while more complex, deliver meaningful robustness advantages under temporal and structured adversarial regimes.
    \item Careful architectural and training design — including regularisation and solver stability — is essential in real-world deployments where adversarial inputs cannot be ruled out.
\end{enumerate}
