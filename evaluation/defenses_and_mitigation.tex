\chapter{Defences and Mitigation Strategies}

\section{Introduction}

The results presented in the \textit{Evaluation} chapter demonstrate that all four architectures - LSTM, TCN, and LNN - are susceptible to adversarial perturbations, albeit to varying degrees and under distinct conditions. This motivates the development of mitigation strategies tailored to each model’s architectural features and the nature of the threats they face.

This chapter explores defence mechanisms aimed at improving robustness without significantly compromising model accuracy or computational efficiency. The focus is placed on strategies that can be realistically integrated into the training or deployment pipelines of temporal models. We divide these methods into three broad categories:

\begin{enumerate}
    \item \textbf{Adversarial Training and Noise Injection:} Involving model exposure to adversarial or noisy inputs during training to promote robustness through experience.
    
    \item \textbf{Architectural Enhancements:} Incorporating inductive biases or structural features that naturally resist perturbation (e.g., gating, memory smoothing, continuous-time stability).
    
    \item \textbf{Input Preprocessing and Filtering:} Applying transformation or filtering to incoming sequences to reduce the effect of adversarial distortions before model ingestion.
\end{enumerate}

Additionally, model-specific recommendations are discussed based on failure modes identified in previous experiments. The chapter concludes with limitations of the proposed defences and potential avenues for future exploration.

\section{Adversarial Training and Noise Injection}

Adversarial training is one of the most widely adopted and empirically effective defence mechanisms against adversarial attacks. The core idea is to augment the training set with adversarially perturbed inputs, thereby exposing the model to a broader range of possible inputs and encouraging robustness via risk minimisation over perturbed distributions.

\subsection{Gradient-Based Adversarial Training}

For attacks such as FGSM or PGD, adversarial examples can be generated on-the-fly during training:
\[
x^{\text{adv}} = x + \epsilon \cdot \text{sign}(\nabla_x \mathcal{L}(f(x), y))
\]
These perturbed inputs are then used in place of or alongside clean data. The modified training objective becomes:
\[
\min_\theta \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \max_{\| \delta \|_\infty \leq \epsilon} \mathcal{L}(f_\theta(x + \delta), y) \right]
\]

\textbf{Implementation:}  
A single-step FGSM was used during training epochs with $\epsilon = 0.03$. For LSTM and TCN, adversarial samples were computed per batch. For the LNN, samples were generated via small perturbations in the ODE input sequence across integration steps.

\subsection{Noise Injection During Training}

In situations where gradients are unavailable (e.g., for black-box threats like SPSA), a more general defence is Gaussian or uniform noise injection:
\[
x^{\text{noisy}} = x + \eta, \quad \eta \sim \mathcal{U}(-\sigma, \sigma)
\]

This method encourages smoother model responses and reduces sensitivity to input fluctuations. While it does not guarantee robustness against worst-case perturbations, it offers a computationally efficient approximation to adversarial training.

\subsection{Benefits and Trade-offs}

\begin{itemize}
    \item \textbf{LSTM:} Adversarial training improved robustness to PGD and FGSM, but caused slower convergence and minor degradation on clean data.
    \item \textbf{TCN:} Showed improved tolerance to localised noise and DeepFool attacks when trained with input noise.
    \item \textbf{LNN:} Incorporating continuous-time noise led to marginal performance gains, but introduced instability unless the ODE solver was finely tuned.
\end{itemize}

\subsection{Considerations}

\begin{itemize}
    \item Adversarial training is computationally intensive, especially with multi-step attacks like PGD.
    \item Excessive noise can underfit the model or blur important signal features.
    \item Robustness gains are often attack-specific and may not generalise to unseen perturbation strategies.
\end{itemize}

Nonetheless, adversarial training remains the most principled and empirically validated defence available, especially when adapted to the architectural properties of the model under consideration.

\section{Architectural Enhancements for Robustness}

Beyond training-based approaches, the design of the model architecture itself plays a pivotal role in determining its robustness characteristics. This section explores structural features and inductive biases that can increase resistance to adversarial perturbations.

\subsection{Memory Mechanisms and Temporal Smoothing}

\textbf{LSTM:} The gating mechanisms in LSTMs, particularly the forget and input gates, provide implicit filtering of noisy inputs. However, this temporal memory also accumulates adversarial errors. Enhancing robustness can involve:
\begin{itemize}
    \item \textit{Adding regularisation on gate activations} to prevent overly sharp transitions.
    \item \textit{Constraining hidden state magnitude} to reduce sensitivity to perturbation propagation.
\end{itemize}

\textbf{Improvement Attempted:} A variant was trained with tanh activation clipped to a reduced range and with cell state clipping. This dampened adversarial degradation but also reduced expressivity.

\subsection{Receptive Field and Feature Redundancy}

\textbf{TCN:} Increasing kernel size or dilation in TCNs extends the receptive field, allowing the model to rely less on any single input timestep. However, this introduces a trade-off between temporal locality and smoothing.

\textbf{Enhancements Explored:}
\begin{itemize}
    \item \textit{Wider convolutional layers} with skip connections were evaluated.
    \item \textit{Dropout in intermediate layers} helped regularise responses to perturbed segments.
\end{itemize}

\subsection{Stability in Continuous-Time Models}

\textbf{LNN:} The liquid neuron architecture is inherently sensitive to solver dynamics and the non-linear state evolution governed by:
\[
\frac{dv_i}{dt} = -\frac{v_i}{\tau} + \sum_j W_{ij} \cdot \sigma(v_j(t)) + u_i(t)
\]
Small perturbations in $u_i(t)$ (input current) may be exponentially amplified depending on $\tau$ and the nonlinearity.

\textbf{Defensive Modifications:}
\begin{itemize}
    \item \textit{Learned decay constants ($\tau$):} Provided adaptive temporal smoothing.
    \item \textit{Bounded activation dynamics:} Capped voltage magnitudes to restrict state drift.
    \item \textit{Solver parameter tuning:} Reduced step size in ODE solver during inference to improve numerical stability under adversarial inputs.
\end{itemize}

\subsection{Summary}

While architectural defences do not eliminate the need for adversarial training, they can significantly reduce sensitivity to certain classes of perturbation. The most robust models observed were those that combined structural filtering (e.g., LNN’s dynamics or TCN’s dilation) with regularised training procedures.

\section{Input Preprocessing and Temporal Defences}

In many practical deployments, direct modification of model architecture or training regime may not be feasible — particularly in black-box or legacy systems. In such scenarios, input preprocessing serves as a lightweight first line of defence. These methods aim to attenuate adversarial perturbations before they reach the model.

\subsection{Low-Pass Filtering}

Adversarial noise, particularly from attacks like FGSM or PGD, often manifests as high-frequency fluctuations. Applying a temporal low-pass filter helps suppress these deviations:
\[
x_t^{\text{filtered}} = \alpha x_t + (1 - \alpha) x_{t-1}
\]
with $\alpha \in [0, 1]$ controlling the smoothing factor.

\textbf{Results:} For TCN and LSTM, this filter reduced degradation from SPSA and PGD by 10–15\%, with minimal impact on clean performance when $\alpha = 0.7$.

\subsection{Interpolation and Resampling Defences}

To mitigate time-warping attacks, one effective method is to resample the input sequence using cubic spline interpolation or uniform temporal alignment:
\begin{itemize}
    \item \textit{Spline interpolation} approximates a smooth underlying trajectory, effectively de-warping irregular temporal spacing.
    \item \textit{Window averaging} across short temporal spans also mitigates local warping effects.
\end{itemize}

\textbf{Effectiveness:} These defences improved robustness for the LSTM and TCN under time-warping attacks, though they occasionally smoothed out meaningful curvature in the data.

\subsection{Temporal Quantisation}

Another strategy is to quantise time input features or sequence positions into discrete buckets. This has the effect of making the model invariant to small timing shifts.

\[
\text{quantised}_t = \left\lfloor \frac{t}{\Delta t} \right\rfloor
\]

\textbf{LNN-Specific Observation:} Quantisation of the time input in the LNN led to an increase in robustness under continuous-time perturbation, at the cost of slight degradation in prediction precision.

\subsection{Trade-offs and Limitations}

\begin{itemize}
    \item \textbf{Pros:} These defences are simple to implement, model-agnostic, and computationally inexpensive.
    \item \textbf{Cons:} They may blunt model sensitivity to meaningful patterns (over-smoothing), and cannot address targeted directional attacks (e.g., DeepFool).
\end{itemize}

\subsection{Summary}

Preprocessing defences act as effective first-pass filters, particularly against noisy or temporally distorted adversarial inputs. When used in conjunction with adversarial training or robust architectures, they form a layered defence approach that improves practical resilience without requiring model retraining.

\section{Model-Specific Mitigation Insights}

Drawing on the analysis from earlier sections, this part distils model-specific insights for robustifying each architecture under adversarial conditions. Each model exhibits unique structural vulnerabilities, which imply different priorities and strategies for defence.

\subsection{LSTM: Sequential Memory Vulnerabilities}

\textbf{Key Weakness:} Early perturbations propagate and amplify through hidden and cell states, leading to long-term prediction errors.

\textbf{Recommended Defences:}
\begin{itemize}
    \item \textbf{Adversarial training with PGD} to harden cell states against gradient-driven perturbation.
    \item \textbf{Cell state clipping} and \textbf{gate activation regularisation} to dampen accumulation of adversarial gradients.
    \item \textbf{Low-pass input filtering} to suppress sharp fluctuations in early inputs.
\end{itemize}

\textbf{Effectiveness:} These mitigations reduced degradation under FGSM and PGD by up to 25\% while preserving validation accuracy.

\subsection{TCN: Localised Perturbation Sensitivity}

\textbf{Key Weakness:} Lack of memory prevents recovery from mid-sequence perturbation; highly sensitive to local distortions in receptive field.

\textbf{Recommended Defences:}
\begin{itemize}
    \item \textbf{Dilation and skip connections} to increase redundancy and global context.
    \item \textbf{Input noise injection} during training to improve robustness to black-box attacks.
    \item \textbf{Temporal interpolation or padding} to desensitise the model to local sequence offsets.
\end{itemize}

\textbf{Effectiveness:} Most robust when combined with uniform noise injection and small kernel-size smoothing filters.

\subsection{LNN: ODE Sensitivity and Stability Management}

\textbf{Key Weakness:} Sensitive to perturbations injected at multiple solver substeps; behaviour governed by dynamics and solver configuration.

\textbf{Recommended Defences:}
\begin{itemize}
    \item \textbf{Continuous-time adversarial training} to mimic dynamic perturbation conditions.
    \item \textbf{Stability regularisation:} Penalising fast state transitions or extreme membrane potentials.
    \item \textbf{Reduced solver step size} at inference time to attenuate numerical instability.
\end{itemize}

\textbf{Effectiveness:} Robustness improved significantly under continuous-time and time-warping attacks when combining dynamic training and solver tuning.

\subsection{Summary Table}

\begin{table}[H]
\centering
\begin{tabular}{|l|p{9cm}|}
\hline
\textbf{Model} & \textbf{Effective Defences} \\
\hline
LSTM & PGD adversarial training, state clipping, input smoothing \\
TCN & Noise injection, receptive field dilation, temporal resampling \\
LNN & Solver tuning, ODE-stability regularisation, continuous-time training \\
\hline
\end{tabular}
\caption{Summary of recommended mitigation strategies by model.}
\label{tab:model_defences}
\end{table}

These insights may serve as practical guidance for model deployment in adversarial environments, especially in real-time or safety-critical systems where robustness cannot be assumed.

\section{Limitations and Future Work}

While the defence strategies outlined in this chapter demonstrate measurable improvements in robustness, several limitations remain. These warrant caution in interpretation and suggest important directions for further research.

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Attack-Specific Optimisation:} Many defences, particularly adversarial training, are tuned to specific attack types. As such, gains may not generalise to novel or unseen perturbation methods.
    
    \item \textbf{Evaluation Scope:} Although a range of attacks was considered, the evaluation was performed on a synthetic 2D spiral task. Generalising these findings to high-dimensional, real-world data (e.g., speech, motion trajectories) requires further validation.

    \item \textbf{Computational Overhead:} Adversarial training and continuous-time solver tuning introduce substantial computational costs, particularly for models like the LNN with dense state transitions.

    \item \textbf{Architectural Rigidity:} Some defences require significant changes to model internals (e.g., solver parameters, memory clipping), which may not be compatible with pre-trained or black-box models.
\end{itemize}

\subsection{Future Work}

\begin{itemize}
    \item \textbf{Robustness Certification:} Incorporating formal verification methods (e.g., symbolic interval analysis or Lipschitz bounding) can provide guarantees under specific perturbation budgets and help validate empirical robustness.

    \item \textbf{Adaptive Defences:} Future work may explore dynamic defence strategies that modulate based on detected input irregularities — such as adaptive smoothing or online solver step adjustment in LNNs.

    \item \textbf{Hybrid Architectures:} Integrating LNNs with recurrent or attention-based modules may improve robustness without sacrificing long-term memory or expressivity.

    \item \textbf{Benchmarking on Real Data:} Applying these defences to real-world tasks, such as physiological signal prediction or time-series classification, would test their practical impact and scalability.

    \item \textbf{Defence-Aware Attacks:} Future research should evaluate robustness under adaptive attackers that account for known defences, offering a more realistic assessment of model security in adversarial environments.
\end{itemize}

\subsection{Conclusion}

The defences presented herein offer a diverse toolbox for enhancing robustness in temporal models. However, robust machine learning remains an adversarial game: as defences evolve, so too do attack strategies. The pursuit of architectures and training regimes that remain stable under dynamic, uncertain, or malicious inputs remains a central challenge in deploying neural systems safely and reliably.