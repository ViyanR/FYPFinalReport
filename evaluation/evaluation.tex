\chapter{Evaluation}
\label{chap:evaluation}
This chapter covers the quantitative and qualitative evaluation of all four models (LNN, TCN, LSTM, and Transformer) on the same input under various adversarial conditions. We systematically assess how each architecture responds to different types of perturbations, focusing on both performance degradation and qualitative failure modes. These metrics are chosen to capture both the accuracy of trajectory predictions and the model's stability and response to input changes.

\section{Evaluation Metrics}

\subsubsection*{1. Mean Squared Error (MSE)}
The loss function used during training was the Mean Squared Error, given by:
\[
\text{MSE} = \frac{1}{T} \sum_{t=1}^{T} \| \hat{x}_t - x_t \|_2^2
\]
where $\hat{x}_t$ is the predicted output at time $t$, and $x_t$ is the ground truth. MSE measures the average difference between predicted and true trajectories, and is used as a baseline measure of performance under clean (non-adversarial) conditions.

\subsubsection*{2. Degradation Ratio}
To evaluate adversarial impact, the degradation ratio is defined as:
\[
\text{Degradation} = \frac{\text{MSE}_{\text{adv}} - \text{MSE}_{\text{clean}}}{\text{MSE}_{\text{clean}} + \delta}
\]
where $\delta$ is a small constant added to avoid division by zero. This metric captures the relative performance decrease caused by adversarial perturbations. This helps to compare vulnerability across models regardless of their baseline MSE.

\subsubsection*{3. Deviation Distance}
The $\ell_2$ deviation between the clean and adversarial predictions is calculated as:
\[
\text{Deviation} = \frac{1}{T} \sum_{t=1}^{T} \| \hat{x}_t^{\text{adv}} - \hat{x}_t^{\text{clean}} \|_2
\]
This metric quantifies the visible divergence in predicted trajectories.

Unlike degradation ratio, which depends on the ground truth, this metric focuses purely on how much the model's output changes under perturbation. It is a task-independent measure of functional instability, showing how sensitive the model's outputs are to small adversarial changes.

\subsubsection*{4. Local Sensitivity (Lipschitz Estimate)}
To characterise the smoothness of the model's function, local sensitivity is estimated by:
\[
\text{Sensitivity} = \frac{\| \hat{x}^{\text{adv}} - \hat{x}^{\text{clean}} \|_2}{\| x^{\text{adv}} - x^{\text{clean}} \|_2}
\]
This ratio approximates the local Lipschitz constant, capturing how much the output changes in response to small input perturbations. A higher sensitivity indicates that the model has sharp local gradients, potentially making it more brittle. This provides a theoretical metric for robustness, independent of task-specific loss.

\section{Quantitative Results}

\subsection*{Aggregate Metrics Across All Attacks}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Avg. Degradation} & \textbf{Avg. Deviation} & \textbf{Lipshchitz Estimate} & \textbf{Clean MSE} \\
\hline
LNN         & ? & ? & ? & 0.002795 \\
TCN        & ? & ? & ? & 0.002782 \\
LSTM         & ? & ? & ? & 0.004416 \\
Transformer & ? & ? & ? & 0.007677 \\ 
\hline
\end{tabular}
\caption{Average degradation and deviation metrics across all attack types.}
\label{tab:agg_metrics}
\end{table}

Importantly, each model was designed/trained to have comparable MSEs, to allow for fair evaluation.

\subsection*{Attack-Specific Results}

The following table summarises the degradation ratios for each model under various adversarial attacks. Lower values indicate better robustness.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{|l|cccc|}
    \hline
    \textbf{Attack Type} & \multicolumn{4}{c|}{\textbf{Degradation (\%)}} \\
    \cline{2-5}
     & \textbf{LNN} & \textbf{TCN} & \textbf{LSTM} & \textbf{Transformer} \\
    \hline
    FGSM                     & 209.6842 & 242.5555 & 169.5259 & 122.8652 \\
    PGD                      & 209.6700 & 241.7085 & 169.5408 & 127.9633 \\
    DeepFool-inspired        & 12.5395 & 13.9643 & 11.2735 & 8.8294 \\
    SPSA                     & 36.8717 & 47.0328 & 29.7149 & 21.6053 \\
    Time-Warping             & 1576.5198 & 1664.5960 & 1033.6341 & 579.3263 \\
    Continuous-Time Perturb. & 458.7058 & 435.9864 & 329.8787 & 242.7071 \\
    \hline
    \end{tabular}
    \caption{Degradation ratios across models for each attack}
    \label{tab:attack_results_degradation}
\end{table}

The following table summarises the average deviation distances for each model under various adversarial attacks. Lower values indicate less deviation from the clean trajectory.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{|l|cccc|}
    \hline
    \textbf{Attack Type} & \multicolumn{4}{c|}{\textbf{Deviation}} \\
    \cline{2-5}
     & \textbf{LNN} & \textbf{TCN} & \textbf{LSTM} & \textbf{Transformer} \\
    \hline
    FGSM                     & 1.293169 & 1.419442 & 1.367006 & 1.474218 \\
    PGD                      & 1.293832 & 1.416078 & 1.367387 & 1.527660 \\
    DeepFool-inspired        & 0.096296 & 0.101111 & 0.111709 & 0.148173 \\
    SPSA                     & 0.777944 & 0.876160 & 0.842481 & 0.919706 \\
    Time-Warping             & 6.953865 & 7.026645 & 6.882243 & 7.036172 \\
    Continuous-Time Perturb. & 2.634408 & 2.599266 & 2.601533 & 2.717356 \\
    \hline
    \end{tabular}
    \caption{Deviation scores across models for each attack}
    \label{tab:attack_results_deviation}
\end{table}

The following table summarises the local sensitivity estimates for each model under various adversarial attacks. Lower values indicate smoother, more robust models.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{|l|cccc|}
    \hline
    \textbf{Attack Type} & \multicolumn{4}{c|}{\textbf{Local Sensitivity (Lipshitz Estimate)}} \\
    \cline{2-5}
     & \textbf{LNN} & \textbf{TCN} & \textbf{LSTM} & \textbf{Transformer} \\
    \hline
    FGSM                     & 0.915554 & 1.004954 & 0.967830 & 1.043735 \\
    PGD                      & 0.919326 & 1.005099 & 0.968100 & 1.090577 \\
    DeepFool-inspired        & 0.962984 & 1.011127 & 1.117503 & 1.497606 \\
    SPSA                     & 0.857096 & 1.003862 & 0.964692 & 1.026772 \\
    Time-Warping             & 0.999767 & 1.010231 & 0.989470 & 1.011601 \\
    Continuous-Time Perturb. & 1.011938 & 1.006981 & 1.005773 & 1.053375 \\
    \hline
    \end{tabular}
    \caption{Lipshitz Estimate (local sensitivity) across models for each attack}
    \label{tab:attack_results_sensitivity}
\end{table}

\hl{ANALYSE THESE TABLES + BAR CHARTS + QUANTITATIVE RESULTS HERE}

\section{Visual Analysis}

While quantitative metrics provide a summary view of model degradation, they can obscure the qualitative character of errors, such as spiralling divergence, phase drift, or geometric distortion. In this section, we present visual comparisons between clean and adversarial predictions to better understand how each model's internal representation and output trajectory is disrupted.

\subsection*{Visualisation Methodology}

For each attack and model combination, clean and adversarial predictions were overlaid on the same plot.
Ground truth trajectories were shown for reference, and all sequences were denormalised prior to plotting.

Each figure highlights a specific failure mode characteristic to the architecture under consideration.

\subsection*{FGSM Attack Results}

The FGSM attack was applied to all three models under a fixed perturbation budget $\epsilon = 0.05$. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/fgsm_spiral_LTC.png}
        \label{fig:fgsm_spiral_LTC}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/fgsm_spiral_TCN.png}
        \label{fig:fgsm_spiral_TCN}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/fgsm_spiral_lstm.png}
        \label{fig:fgsm_spiral_lstm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/fgsm_spiral_transformer.png}
        \label{fig:fgsm_spiral_transformer}
    \end{subfigure}
    \caption{Predicted, Target, and Adversarial projections under FGSM attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}
    \label{fig:fsgm_spirals}
\end{figure}

From figure~\ref{fig:fsgm_spirals}, the Liquid Neural Network (LTC) shows notable resilience, maintaining close alignment between its original and adversarial predictions for the majority of the trajectory. While minor deviations are visible, especially near the inner spiral, the global structure remains coherent. This is likely due to the continuous-time membrane integration and temporal smoothing induced by the ODE solver.

In contrast, the TCN and Transformer models show pronounced adversarial distortion. The TCN suffers from its lack of recurrent memory, leading to sharp deflections in regions of local perturbation.

The Transformer, despite its capacity, exhibits highly unstable predictions under attack, likely due to its limited inductive bias and sensitivity to global input shifts.

The LSTM model shows intermediate robustness: although its gating structure filters out some perturbations, it remains vulnerable to gradient-based attacks that propagate through long-term memory.

Thus, the LTC's continuous dynamics and biophysical parameterisation seem to have stabilising effect, outperforming the more discrete or attention-based baselines under single-step adversarial pressure.

\subsection*{PGD Attack Results}

PGD, as a stronger attack method, caused greater degradation than FGSM across all models, with stronger effect on deeper temporal structures.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/pgd_spiral_LTC.png}
        \label{fig:pgd_spiral_LTC}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/pgd_spiral_TCN.png}
        \label{fig:pgd_spiral_TCN}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/pgd_spiral_lstm.png}
        \label{fig:pgd_spiral_lstm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/pgd_spiral_transformer.png}
        \label{fig:pgd_spiral_transformer}
    \end{subfigure}
    \caption{Predicted, Target, and Adversarial projections under PGD attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}
    \label{fig:pgd_spirals}
\end{figure}

Figure~\ref{fig:pgd_spirals} presents the effect of a 15-step PGD attack on each model, using $\epsilon = 0.05$ and $\alpha = 0.01$. The LTC continues to exhibit strong robustness, with its adversarial trajectory (blue) closely shadowing the original prediction (red) and the true path (green). Despite minor divergence being visible (particularly near the spiral's centre where curvature is high) the global structure remains largely preserved. This resilience is again due to the regularising effect of continuous-time dynamics and the internal ODE solver. For the PGD attack, this limits perturbations across integration steps, dampening the cumulative effect.

In contrast, the Transformer model suffers substantial degradation. Its adversarial trajectory spirals off course significantly, as a result of the model's sensitivity to small input shifts and lack of temporal inductive bias.

The LSTM also displays clear distortion, particularly in early spiral turns, reflecting how perturbations to the input cascade through long-range memory states. 

The TCN, while affected, shows slightly improved robustness relative to the Transformer and LSTM, likely due to the inherent smoothing effect of its dilated convolutions. The absence of state memory however limits its ability to resist cumulative perturbation.

The LTC model still seems to stabilise stabilise predictions under iterative, gradient-based adversaries like PGD.

\subsection*{Deepfool-Like Attack Results}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/deepfool_spiral_LTC.png}
        \label{fig:deepfool_spiral_LTC}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/deepfool_spiral_TCN.png}
        \label{fig:deepfool_spiral_TCN}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/deepfool_spiral_lstm.png}
        \label{fig:deepfool_spiral_lstm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/deepfool_spiral_transformer.png}
        \label{fig:deepfool_spiral_transformer}
    \end{subfigure}
    \caption{Predicted, Target, and Adversarial projections under Deepfool-Like attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}
    \label{fig:deepfool_spirals}
\end{figure}

The plots in Figure~\ref{fig:deepfool_spirals} show the effects of a DeepFool-like iterative attack (20 steps, step size = 0.005) adapted for regression. Unlike PGD or FGSM, which aim for maximal distortion within a norm-bound, DeepFool attempts to perturb inputs just enough to cross decision boundaries. In this case, this corresponds to inducing maximal deviation in continuous predictions. The Transformer model shows the highest vulnerability: its adversarial trajectory (blue) diverges significantly from the true path (green), especially in the inner spiral turns where minor distortions amplify rapidly.

The LSTM is similarly affected, to a slightly lesser extent, with perturbations distorting predictions in both early and late sequence segments. This aligns with known vulnerability of memory-based architectures to accumulated gradient signals across time.

The TCN demonstrates moderate resilience. While some deviations occur, especially near tight curvatures, the global shape is retained.

The LTC model remains the most stable under this attack. Its adversarial predictions are closer to the original (red), indicating that the internal ODE dynamics of the LNN may diffuse perturbations temporally, flattening adversarial gradients.

\subsection*{SPSA Results}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/spsa_spiral_LTC.png}
        \label{fig:spsa_spiral_LTC}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/spsa_spiral_TCN.png}
        \label{fig:spsa_spiral_TCN}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/spsa_spiral_lstm.png}
        \label{fig:spsa_spiral_lstm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/spsa_spiral_transformer.png}
        \label{fig:spsa_spiral_transformer}
    \end{subfigure}
    \caption{Predicted, Target, and Adversarial projections under SPSA attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}
    \label{fig:spsa_spirals}
\end{figure}

As shown in Figure~\ref{fig:spsa_spirals}, the impact of SPSA is comparatively mild across all models, consistent with its tendency to explore less optimal perturbation directions relative to gradient-based attacks like PGD. It does still reveal architectural differences in robustness.

The LTC model shows minimal divergence under SPSA, maintaining tight alignment between original and adversarial predictions. This further reinforces it's resilience observed in prior attack scenarios.

The TCN and Transformer show more visible degradation in the spiral's outer loops, indicating that their fixed receptive fields and positional encoding mechanisms are more easily disrupted by uniform input noise. 

The LSTM shows moderate vulnerability, with adversarial traces deviating steadily from the true trajectory over time.

\subsection*{Time Warping Attack Results}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/time_warping_spiral_LTC.png}
        \label{fig:time_warping_spiral_LTC}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/time_warping_spiral_TCN.png}
        \label{fig:time_warping_spiral_TCN}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/time_warping_spiral_lstm.png}
        \label{fig:time_warping_spiral_lstm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/time_warping_spiral_transformer.png}
        \label{fig:time_warping_spiral_transformer}
    \end{subfigure}
    \caption{Predicted, Target, and Adversarial projections under Time Warping attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}
    \label{fig:time_warping_spirals}
\end{figure}

Figure~\ref{fig:time_warping_spirals} displays the impact of a time-warping attack on each model. This attack perturbs the temporal spacing of input points without altering the values themselves, distorting the perceived dynamics of the sequence. The LSTM model appears particularly sensitive, with adversarial predictions deviating noticeably from the true path green, especially in outer spiral turns. This is expected given the LSTM's reliance on a fixed temporal memory structure, which is easily disrupted by misaligned temporal cues.

Similarly, the Transformer struggles to adapt to the warped temporal signal, showing severe prediction drift. This is due to its reliance on position encodings that assume uniform spacing.

In contrast, the TCN maintains good alignment with the true path, showing resilience due to its convolutional structure and dilated receptive fields, which provide a form of local smoothing.

The Liquid Neural Network (LTC) performs best under this attack. Its continuous-time dynamics and internal state integration appear to absorb the warping, maintaining alignment between the adversarial and original predictions. This reinforces the hypothesis that ODE-based temporal models like LNNs are inherently more invariant to temporal distortions, making them better suited for tasks with irregular or corrupted time sampling.

\subsection*{Continuous-Time Adversarial Perturbation Attack Results}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/continuous_time_spiral_LTC.png}
        \label{fig:continuous_time_spiral_LTC}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/continuous_time_spiral_TCN.png}
        \label{fig:continuous_time_spiral_TCN}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/continuous_time_spiral_lstm.png}
        \label{fig:continuous_time_spiral_lstm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{img/continuous_time_spiral_transformer.png}
        \label{fig:continuous_time_spiral_transformer}
    \end{subfigure}
    \caption{Predicted, Target, and Adversarial projections under Continuous-Time Adversarial Perturbation attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}
    \label{fig:continuous_time_spirals}
\end{figure}

The continuous-time perturbation attack, specifically tailored for models governed by differential equations, targets the latent state evolution rather than directly modifying the input trajectory. This makes it particularly suited for evaluating the Liquid Neural Network (LTC), whose continuous-time membrane dynamics offer both flexibility and potential vulnerability to such perturbations.

As shown in Figure~\ref{fig:continuous_time_spirals}, the LTC model remains relatively stable under attack, with adversarial predictions closely tracking the original predictions throughout the spiral. This indicates that the internal ODE dynamics, when properly regularised, can absorb structured noise and maintain functional predictions.

The TCN and LSTM models however exhibit more pronounced deviations, especially in the outer spiral turns, suggesting that perturbations which indirectly affect temporal relationships (e.g. via integration-like behaviour) are disruptive to their sequential representations.

The Transformer, which lacks an explicit temporal evolution mechanism, is highly vulnerable to these perturbations, with the adversarial trajectory diverging significantly from the true path.

This again highlights a key advantage of biologically inspired continuous-time models (LNNs) in handling dynamic, structured adversarial noise.

\subsection*{Interpretation}

Models with rigid temporal assumptions or recurrent memory (LSTM) are more susceptible to both magnitude and timing distortions, whereas continuous-time dynamics (LNN) offer meaningful resistance. However, no model was universally robust, and each architecture exhibited specific weaknesses when faced with particular perturbation types.

\section{Further Analysis}

\subsection*{Comparative Failure Modes}

\begin{itemize}
    \item \textbf{LSTM:} Most errors are due to memory misalignment; adversarial perturbations early in the sequence affect long-term predictions.
    \item \textbf{TCN:} Exhibits immediate, localised distortions that do not propagate. However, global structure is harder to recover post-perturbation.
    \item \textbf{LNN:} Shows resilience to smooth temporal shifts but is vulnerable to persistent directional gradients or rapidly fluctuating noise.
\end{itemize}

\subsection*{Phase Drift and Spiral Collapse}

A recurring theme observed across all models under PGD and DeepFool-like attacks is \emph{phase drift} — a steady deviation in angular position on the spiral. Unlike random noise, these attacks produce a consistent directional bias, causing the prediction to spiral inward or outward.

\hl{spiral\_phase\_drift image here}

\subsection*{Interpretive Summary}

Visual inspection confirms that degradation is not uniform:
\begin{itemize}
    \item Some attacks (e.g., PGD, directional gradient) cause persistent trajectory drift.
    \item Others (e.g., SPSA, FGSM) introduce transient but recoverable perturbations.
    \item Architectures with memory (LSTM) are vulnerable to compounding errors; feedforward models (TCN) localise degradation; ODE-based models (LNN) smooth over it.
\end{itemize}

These insights are not easily captured by scalar error metrics alone and reinforce the importance of including visual diagnostics in robustness evaluation.

\section{Comparative Discussion of Model Robustness}

Having evaluated the LNN, TCN, and LSTM across a wide spectrum of adversarial conditions, this section synthesises key observations into a comparative robustness profile. The aim is not only to rank models by resistance but to understand \emph{why} certain architectures fail or succeed under specific types of perturbation.

\subsection{Summary of Behaviour Under Attack}

\begin{itemize}
    \item \textbf{LSTM:} Performs well under clean conditions, but suffers sharp degradation when adversarial noise is injected early in the sequence. The accumulation of errors in its gated memory mechanisms makes it particularly vulnerable to directional attacks (e.g., PGD, DeepFool). Despite this, it displays limited robustness to noise-based attacks like SPSA.
    
    \item \textbf{TCN:} Its feedforward and convolutional architecture gives it moderate robustness across most attacks. TCNs are especially vulnerable to non-local attacks like PGD that exploit the full sequence context, but are relatively stable under local noise and gradient-free attacks (e.g., SPSA). However, the model lacks a temporal memory mechanism to re-anchor itself after an attack.

    \item \textbf{LNN:} Exhibits the most consistent robustness, particularly under time-warping and continuous-time attacks. Its ODE-based internal state provides smoother transitions and better filtering of high-frequency noise. Nevertheless, the LNN is not invulnerable—attacks that align with sensitive dynamical regimes (e.g., PGD or high-amplitude SPSA) can still destabilise the model.
\end{itemize}

\subsection*{Interpretation of Results}
No single model outperformed others under all adversarial settings. The LSTM's gating mechanisms offered some regularisation benefits but failed under directional and temporal distortions. The TCN was resilient to localised noise but vulnerable to global shifts and multi-step attacks. The LNN demonstrated nuanced robustness, especially against temporal distortions, but remained sensitive to high-frequency injected noise within its ODE solver.

Overall, the diversity of attack types reveals how robustness is not a singular property but a complex interplay of architectural assumptions, dynamic behaviour, and model training dynamics.

\vspace{1em}
\noindent The next chapter explores these architectural and behavioural insights in greater depth by comparing model robustness across all attacks using quantitative and qualitative metrics.


\subsection*{Architectural Trade-offs}

Each model's robustness can be linked to its architectural assumptions:
\begin{enumerate}
    \item \textbf{LSTM:} Sequential dependence and gating offer rich temporal modelling but also amplify error propagation. This makes them unsuitable for tasks where adversarial access to early inputs is likely.
    \item \textbf{TCN:} Its parallel structure and limited receptive field enable stable training and efficiency, but prevent long-term correction after perturbation. It is highly sensitive to the location of the attack.
    \item \textbf{LNN:} By encoding time explicitly through continuous dynamics, the LNN achieves robustness to subtle perturbations in both time and space. However, stability depends heavily on solver configuration and the nonlinearity of the governing ODE.
\end{enumerate}

\subsection*{Robustness by Attack Type}

\begin{itemize}
    \item \textbf{Gradient-based attacks:} PGD and DeepFool-inspired attacks exploit local curvature in the loss landscape. LSTM suffers most due to deep recurrence. LNN partially resists due to its low-sensitivity ODE integration.
    \item \textbf{Gradient-free attacks:} SPSA shows that even in black-box settings, models like the TCN can be significantly affected by repeated local perturbations.
    \item \textbf{Temporal attacks:} Time-warping and continuous-time perturbations target the model's implicit assumptions about sampling frequency and state evolution. LNN outperforms others, showcasing a key advantage of continuous-time architectures in adversarial settings.
\end{itemize}

\subsection*{Implications for Deployment}

These findings carry important implications:
\begin{itemize}
    \item When deploying models in adversarial or uncertain environments, the temporal assumptions of the architecture must be scrutinised.
    \item Robustness is context-dependent — no model is universally secure, and the choice of architecture should be informed by the anticipated type of input perturbation.
    \item LNNs offer promising directions for tasks where input timing is noisy or attacker-controlled, such as sensor-based monitoring or robotics.
\end{itemize}

\section{Defences for Liquid Neural Networks}

This section explores theoretical strategies to mitigate adversarial vulnerability in Liquid Neural Networks (LNNs), with comparisons to TCN and LSTM where relevant. The focus is on defences grounded in continuous-time modelling, structural stability, and principled input processing.

\subsection*{Adversarial Training and Perturbation Smoothing}

While adversarial training has been shown effective in conventional networks \cite{madry2018towards}, its adaptation to continuous-time models like LNNs requires careful consideration. One potential direction is continuous-time adversarial training, where adversarial perturbations are applied not only at the input level but across intermediate ODE solver steps. This would simulate realistic, dynamic threats such as time-warping or sensor noise.

In addition, input noise injection (e.g. Gaussian or uniform noise) could promote smoother input-output mappings, as seen in denoising autoencoders \cite{vincent2008extracting}. For LNNs, perturbing the input current $u_i(t)$ over time can improve resistance to minor temporal distortions.

\subsection*{Structural Regularisation in Liquid Architectures}

A core advantage of LNNs lies in their biologically-inspired dynamics:
\begin{equation}
\frac{dv_i}{dt} = -\frac{v_i}{\tau} + \sum_j W_{ij} \cdot \sigma(v_j(t)) + u_i(t)
\end{equation}

Here, the decay constant $\tau$ and activation function $\sigma$ are learnable. Imposing soft constraints on $\tau$ or bounding the membrane potential $v_i$ could prevent exponential state divergence under perturbation.

\textbf{Figure~\ref{fig:lnn_dynamics_defence}} illustrates how limiting the slope of the state curve over time may preserve stability.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1.0, every node/.style={font=\small}]
        % Axes
        \draw[->] (0,0) -- (6,0) node[right] {Time $t$};
        \draw[->] (0,-2.2) -- (0,2.2) node[above] {Membrane Potential $v_i(t)$};
    
        % Unregularised curve (exponential growth)
        \draw[red, thick, domain=0:5.2, samples=100] plot (\x, {1.5*(1 - exp(-0.7*\x)) + 0.4*sin(2*\x r)});
        \node[red] at (5.5,1.7) {\scriptsize Unregularised};
    
        % Regularised curve (bounded)
        \draw[blue, thick, domain=0:5.2, samples=100] plot (\x, {tanh(1.5*(1 - exp(-0.7*\x)))});
        \node[blue] at (5.5,0.6) {\scriptsize Regularised};
    
        % Horizontal dashed lines (bounds)
        \draw[dashed, gray] (0,1.2) -- (5.7,1.2);
        \draw[dashed, gray] (0,-1.2) -- (5.7,-1.2);
        \node[gray] at (5.9,1.2) {\scriptsize $+v_{\max}$};
        \node[gray] at (5.9,-1.2) {\scriptsize $-v_{\max}$};
    \end{tikzpicture}
    \caption{LNN membrane potential evolution with and without regularisation. Regularisation limits state drift, improving stability under input perturbations.}
    \label{fig:lnn_dynamics_defence}
    \end{figure}

Additionally, dynamically adjusting the solver step size during inference (based on detected input smoothness or gradient norms) can act as another form of defence. Such adaptive schemes are supported by work in neural ODE robustness \cite{dupont2019augmented, finlay2020trainable}.

\subsection*{Temporal Input Filtering and Quantisation}

Preprocessing defences offer architecture-agnostic robustness benefits. Applying a low-pass filter to the input trajectory, defined as:
\begin{equation}
x_t^{\text{filtered}} = \alpha x_t + (1 - \alpha) x_{t-1}, \quad \alpha \in [0, 1]
\end{equation}
can suppress high-frequency adversarial noise typical in gradient-based attacks.

Similarly, discretising the time component or input position encoding:
\begin{equation}
\text{quantised}_t = \left\lfloor \frac{t}{\Delta t} \right\rfloor
\end{equation}
has been hypothesised to improve invariance to small timing shifts \cite{wang2018adversarial}.

% \begin{figure}[H]
% \centering
% \includegraphics[width=0.65\linewidth]{img/temporal_filtering.png}
% \caption{Illustration of low-pass temporal filtering as a defence mechanism. Sharp perturbations are smoothed before reaching the model.}
% \label{fig:temporal_filter}
% \end{figure}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1.0, every node/.style={font=\small}]
        % Axes
        \draw[->] (0,0) -- (6.5,0) node[right] {Time $t$};
        \draw[->] (0,-2.2) -- (0,2.2) node[above] {Input Value};
    
        % Raw input signal (with perturbations)
        \draw[red, thick, domain=0:6, samples=100, smooth] plot (\x, {1.2*sin(1.5*\x r) + 0.5*sin(12*\x r)});
        \node[red] at (6.2,1.5) {\scriptsize Raw Input};
    
        % Filtered signal
        \draw[blue, thick, domain=0:6, samples=100, smooth] plot (\x, {1.2*sin(1.5*\x r)});
        \node[blue] at (6.1,-1.2) {\scriptsize Filtered};
    
        % Noise label
        \draw[->, thick, gray] (2.6,1.2) -- (2.3,0.3);
        \node[gray] at (3.2,1.3) {\scriptsize High-frequency perturbation};
    
        % Filter label
        \draw[->, thick, gray] (1.7,-0.7) -- (1.7,-1.5);
        \node[gray] at (2.4,-0.6) {\scriptsize Smoothed by filter};
    
    \end{tikzpicture}
    \caption{Illustration of low-pass temporal filtering as a defence mechanism. Sharp perturbations are smoothed before reaching the model.}
    \label{fig:temporal_filter}
    \end{figure}

\subsection*{Inductive Bias and Solver-Aware Stability}

Unlike discrete models, LNNs require the solver to faithfully capture non-linear temporal dynamics. This opens unique defence opportunities:
\begin{itemize}
\item \textbf{Step-size modulation:} Smaller solver steps reduce numerical error accumulation under perturbed inputs.
\item \textbf{Activation bounding:} Enforcing $|v_i(t)| \leq V_\text{max}$ could prevent state explosion.
\item \textbf{Implicit regularisation:} Using solver-aware loss terms penalising high curvature over time.
\end{itemize}

These mechanisms can be integrated during training to enforce smooth, stable behaviour even when inputs deviate adversarially.

\subsection*{Limitations}

Liquid models do also present unique challenges, since adversarial attacks in continuous time are underexplored and difficult to formalise. Solver regularisation may reduce model expressivity or increase compute cost. There is also a risk that filtering can remove meaningful signal patterns in real-world tasks.

Practially incorporating formal verification \cite{zhang2022towards} or hybrid models that mix discrete and continuous pathways may further improve LNN reliability.
