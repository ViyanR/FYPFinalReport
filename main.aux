\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{5}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions}{5}{section.1.2}\protected@file@percent }
\citation{hasaniLiquidTimeconstantNetworks2021}
\citation{frank2024learning}
\citation{frank2024learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background and Literature Review}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Liquid Neural Networks}{7}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  Schematic Diagram of a Liquid Neural Network~\cite  {frank2024learning}. Figure illustrates the simplified architecture of an LNN, starting with the input layer receiving time-series data. The data then flows into the liquid layer, where dynamic, non-linear processing occurs through a complex network of interconnected neurons. Finally, the processed information is relayed to the output layer. }}{7}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:lnn_overview}{{2.1}{7}{Schematic Diagram of a Liquid Neural Network~\cite {frank2024learning}. Figure illustrates the simplified architecture of an LNN, starting with the input layer receiving time-series data. The data then flows into the liquid layer, where dynamic, non-linear processing occurs through a complex network of interconnected neurons. Finally, the processed information is relayed to the output layer}{figure.caption.4}{}}
\citation{tedxtalksLiquidNeuralNetworks2023}
\newlabel{eq:1}{{2.1}{8}{Continuous-Time Dynamical Systems/Differential Equations}{equation.2.1.1}{}}
\citation{chahineRobustFlightNavigation2023}
\citation{szegedy2013intriguing}
\citation{goodfellow2014explaining}
\citation{liquidTimeConstant}
\citation{liquidTimeConstant}
\citation{liquidTimeConstant}
\citation{liquidTimeConstant}
\citation{goodfellow2014explaining}
\citation{madry2018towards}
\citation{moosavi2016deepfool}
\citation{uesato2018adversarial}
\citation{cisse2017houdini}
\citation{sun2018natural}
\citation{weng2018evaluating}
\citation{dong2020benchmarking}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Adversarial Attacks}{11}{section.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Adversarial Attack on a timeseries input~\cite  {liquidTimeConstant} \cite  {liquidTimeConstant} }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig:spiking2}{{2.2}{11}{Adversarial Attack on a timeseries input~\cite {liquidTimeConstant} \cite {liquidTimeConstant}}{figure.caption.13}{}}
\citation{henriksenEfficientNeuralNetwork}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Neural Network Verification}{12}{section.2.3}\protected@file@percent }
\newlabel{verification_def}{{2.3}{12}{The Verification Problem}{section*.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Steps in SIP:}{12}{section*.18}\protected@file@percent }
\citation{zhangEfficientNeuralNetwork2018}
\@writefile{toc}{\contentsline {paragraph}{Advantages for Liquid Neural Networks}{13}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Considerations}{13}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mathematical Framework}{13}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bounding Nonlinearities}{13}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Output Certification}{14}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Application to Neural ODEs}{14}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Implementation}{14}{section*.26}\protected@file@percent }
\citation{ltctutorial2022}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Liquid Neural Network Design and Implementation}{15}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Design Overview}{15}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Wiring and Connectivity}{15}{section.3.2}\protected@file@percent }
\citation{hasaniLiquidTimeconstantNetworks2021}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}{\ignorespaces Simplified RandomWiring class}}{16}{lstlisting.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}LTC Neuron Dynamics}{16}{section.3.3}\protected@file@percent }
\newlabel{eq:v_update}{{3.1}{16}{LTC Neuron Dynamics}{equation.3.3.1}{}}
\newlabel{eq:v_update_denominator}{{3.2}{16}{LTC Neuron Dynamics}{equation.3.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Example internal membrane voltage trace across $K$ ODE unfolding steps for a single neuron. This entire graph represents the internal dynamics used to compute $v_i^{(t+1)}$ from $v_i^{(t)}$, in a single timestep.}}{17}{figure.caption.27}\protected@file@percent }
\newlabel{fig:lif_voltage_evolution}{{3.1}{17}{Example internal membrane voltage trace across $K$ ODE unfolding steps for a single neuron. This entire graph represents the internal dynamics used to compute $v_i^{(t+1)}$ from $v_i^{(t)}$, in a single timestep}{figure.caption.27}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.2}{\ignorespaces Simplified LTC neuron forward method}}{18}{lstlisting.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Internal structure of a single LIF neuron used in the Liquid Time-Constant Network. Inputs undergo non-linear transformations based on trainable $\mu $ and $\sigma $, and the resulting activations are integrated using biophysical parameters (leak conductance $g_\text  {leak}$, membrane capacitance $C_m$, and reversal potentials $E_\text  {rev}$).}}{18}{figure.caption.28}\protected@file@percent }
\newlabel{fig:lif_neuron_detailed}{{3.2}{18}{Internal structure of a single LIF neuron used in the Liquid Time-Constant Network. Inputs undergo non-linear transformations based on trainable $\mu $ and $\sigma $, and the resulting activations are integrated using biophysical parameters (leak conductance $g_\text {leak}$, membrane capacitance $C_m$, and reversal potentials $E_\text {rev}$)}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Network Architecture}{18}{section.3.4}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.3}{\ignorespaces Structure of the LTCRNN module}}{19}{lstlisting.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Architecture of the Liquid Time-Constant Network (LNN). Inputs project through learned sensory filters to a sparsely recurrent Liquid Layer of LIF neurons. Dynamics are integrated using an internal ODE solver with unfolding. Final outputs are read from a low-dimensional projection.}}{19}{figure.caption.30}\protected@file@percent }
\newlabel{fig:lnn_architecture}{{3.3}{19}{Architecture of the Liquid Time-Constant Network (LNN). Inputs project through learned sensory filters to a sparsely recurrent Liquid Layer of LIF neurons. Dynamics are integrated using an internal ODE solver with unfolding. Final outputs are read from a low-dimensional projection}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Training Configuration (and dataset)}{19}{section.3.5}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.4}{\ignorespaces Simplified training loop for the LNN}}{20}{lstlisting.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Training Behaviour}{20}{section.3.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Training and validation loss over epochs.}}{21}{figure.caption.33}\protected@file@percent }
\newlabel{fig:lnn_loss}{{3.4}{21}{Training and validation loss over epochs}{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Training and validation loss over epochs (zoomed).}}{21}{figure.caption.34}\protected@file@percent }
\newlabel{fig:lnn_loss_zoomed}{{3.5}{21}{Training and validation loss over epochs (zoomed)}{figure.caption.34}{}}
\newlabel{fig:lnn_training_validation_spiral_epoch_1}{{3.6a}{22}{Epoch 1}{figure.caption.36}{}}
\newlabel{sub@fig:lnn_training_validation_spiral_epoch_1}{{a}{22}{Epoch 1}{figure.caption.36}{}}
\newlabel{fig:lnn_training_validation_spiral_epoch_400}{{3.6b}{22}{Epoch 400}{figure.caption.36}{}}
\newlabel{sub@fig:lnn_training_validation_spiral_epoch_400}{{b}{22}{Epoch 400}{figure.caption.36}{}}
\newlabel{fig:lnn_training_validation_spiral_epoch_2000}{{3.6c}{22}{Epoch 2000}{figure.caption.36}{}}
\newlabel{sub@fig:lnn_training_validation_spiral_epoch_2000}{{c}{22}{Epoch 2000}{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces LLN predicted vs true spiral trajectories across training: early, mid, and final epochs (denormalised training and validation spiral)}}{22}{figure.caption.36}\protected@file@percent }
\newlabel{fig:lnn_spiral_progression_grid}{{3.6}{22}{LLN predicted vs true spiral trajectories across training: early, mid, and final epochs (denormalised training and validation spiral)}{figure.caption.36}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Comparative Models}{23}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction to Baseline Models}{23}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Temporal Convolutional Network (TCN)}{23}{section.4.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.1}{\ignorespaces Simplified TCN architecture}}{24}{lstlisting.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Temporal Convolutional Network (TCN) architecture}}{24}{figure.caption.41}\protected@file@percent }
\newlabel{fig:tcn_architecture}{{4.1}{24}{Temporal Convolutional Network (TCN) architecture}{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Structure of a TCN Residual Block. Each convolution uses dilation to increase receptive field, while residual skip connections and dropout stabilise training.}}{25}{figure.caption.42}\protected@file@percent }
\newlabel{fig:tcn_residual_block}{{4.2}{25}{Structure of a TCN Residual Block. Each convolution uses dilation to increase receptive field, while residual skip connections and dropout stabilise training}{figure.caption.42}{}}
\newlabel{fig:tcn_training_validation_spiral_epoch_1}{{4.3a}{25}{Epoch 1}{figure.caption.45}{}}
\newlabel{sub@fig:tcn_training_validation_spiral_epoch_1}{{a}{25}{Epoch 1}{figure.caption.45}{}}
\newlabel{fig:tcn_training_validation_spiral_epoch_400}{{4.3b}{25}{Epoch 400 (early stopping point)}{figure.caption.45}{}}
\newlabel{sub@fig:tcn_training_validation_spiral_epoch_400}{{b}{25}{Epoch 400 (early stopping point)}{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces TCN predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}}{25}{figure.caption.45}\protected@file@percent }
\newlabel{fig:tcn_spiral_progression_grid}{{4.3}{25}{TCN predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}{figure.caption.45}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Long Short-Term Memory Network (LSTM)}{26}{section.4.3}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.2}{\ignorespaces Simplified LSTM model structure}}{26}{lstlisting.4.2}\protected@file@percent }
\citation{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Architecture of the LSTM model. The left shows residual-enhanced flow through a 2-layer LSTM, while the right shows the LSTM unrolled across time with hidden and cell state transitions.}}{27}{figure.caption.50}\protected@file@percent }
\newlabel{fig:lstm_architecture_final}{{4.4}{27}{Architecture of the LSTM model. The left shows residual-enhanced flow through a 2-layer LSTM, while the right shows the LSTM unrolled across time with hidden and cell state transitions}{figure.caption.50}{}}
\newlabel{fig:lstm_training_validation_spiral_epoch_1}{{4.5a}{27}{Epoch 1}{figure.caption.53}{}}
\newlabel{sub@fig:lstm_training_validation_spiral_epoch_1}{{a}{27}{Epoch 1}{figure.caption.53}{}}
\newlabel{fig:lstm_training_validation_spiral_epoch_1000}{{4.5b}{27}{Epoch 1000 (early stopping point)}{figure.caption.53}{}}
\newlabel{sub@fig:lstm_training_validation_spiral_epoch_1000}{{b}{27}{Epoch 1000 (early stopping point)}{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces LSTM predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}}{27}{figure.caption.53}\protected@file@percent }
\newlabel{fig:lstm_spiral_progression_grid}{{4.5}{27}{LSTM predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}{figure.caption.53}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Transformer Model}{28}{section.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Architecture of the Transformer encoder used}}{29}{figure.caption.57}\protected@file@percent }
\newlabel{fig:transformer_encoder_diagram}{{4.6}{29}{Architecture of the Transformer encoder used}{figure.caption.57}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.3}{\ignorespaces Simplified Transformer architecture}}{29}{lstlisting.4.3}\protected@file@percent }
\newlabel{fig:transformer_training_validation_spiral_epoch_1}{{4.7a}{30}{Epoch 1}{figure.caption.61}{}}
\newlabel{sub@fig:transformer_training_validation_spiral_epoch_1}{{a}{30}{Epoch 1}{figure.caption.61}{}}
\newlabel{fig:transformer_training_validation_spiral_epoch_600}{{4.7b}{30}{Epoch 600 (early stopping point)}{figure.caption.61}{}}
\newlabel{sub@fig:transformer_training_validation_spiral_epoch_600}{{b}{30}{Epoch 600 (early stopping point)}{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Transformer predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}}{30}{figure.caption.61}\protected@file@percent }
\newlabel{fig:transformer_spiral_progression_grid}{{4.7}{30}{Transformer predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}{figure.caption.61}{}}
\citation{goodfellow2015explaining}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Adversarial Attack Methodology and Robustness Certification}{31}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction to Adversarial Attacks}{31}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Fast Gradient Sign Method (FGSM)}{31}{section.5.2}\protected@file@percent }
\citation{madry2018towards}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}{\ignorespaces FGSM adversarial attack implementation}}{32}{lstlisting.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Projected Gradient Descent (PGD)}{32}{section.5.3}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.2}{\ignorespaces PGD Attack Loop (Simplified)}}{32}{lstlisting.5.2}\protected@file@percent }
\citation{moosavi2016deepfools}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}DeepFool-Inspired Directional Attack}{33}{section.5.4}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.3}{\ignorespaces Directional (DeepFool-like) Gradient Attack}}{33}{lstlisting.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Simultaneous Perturbation Stochastic Approximation (SPSA)}{34}{section.5.5}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.4}{\ignorespaces Simplified SPSA implementation}}{34}{lstlisting.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Time-Warping Attack}{34}{section.5.6}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.5}{\ignorespaces Example Time-Warping Attack Function}}{35}{lstlisting.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Continuous-Time Perturbation Attack}{35}{section.5.7}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.6}{\ignorespaces Continuous-Time Perturbation Injection}}{36}{lstlisting.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Summary of Attack Design and Implementation Decisions}{36}{section.5.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Overview of attack types and model sensitivities.}}{36}{table.caption.83}\protected@file@percent }
\newlabel{tab:attack_summary}{{5.1}{36}{Overview of attack types and model sensitivities}{table.caption.83}{}}
\citation{autoLiRPA2022}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Certified Robustness Analysis of the LNN via Interval Bound Propagation}{37}{section.5.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Certified output bounds for LNN prediction on dimension 0, computed using IBP via \texttt  {auto\_LiRPA}. The blue line denotes the nominal output; the pink region denotes the certified interval over \( \ell _\infty \)-bounded perturbations.}}{38}{figure.caption.88}\protected@file@percent }
\newlabel{fig:lnn_ibp_bounds}{{5.1}{38}{Certified output bounds for LNN prediction on dimension 0, computed using IBP via \texttt {auto\_LiRPA}. The blue line denotes the nominal output; the pink region denotes the certified interval over \( \ell _\infty \)-bounded perturbations}{figure.caption.88}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Evolution of certified output bound width over time, for both output dimensions of the LNN. Spikes in width correspond to higher uncertainty or sensitivity in the model's output.}}{38}{figure.caption.90}\protected@file@percent }
\newlabel{fig:bound_width_plot}{{5.2}{38}{Evolution of certified output bound width over time, for both output dimensions of the LNN. Spikes in width correspond to higher uncertainty or sensitivity in the model's output}{figure.caption.90}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Evaluation}{40}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:evaluation}{{6}{40}{Evaluation}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Evaluation Metrics}{40}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Quantitative Results}{41}{section.6.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Average robustness metrics across all attacks. Lower values are better for all columns.}}{41}{table.caption.97}\protected@file@percent }
\newlabel{tab:avg_model_robustness}{{6.1}{41}{Average robustness metrics across all attacks. Lower values are better for all columns}{table.caption.97}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Average robustness metrics by model across all attacks. Lower is better.}}{41}{figure.caption.98}\protected@file@percent }
\newlabel{fig:avg_metrics_barplot}{{6.1}{41}{Average robustness metrics by model across all attacks. Lower is better}{figure.caption.98}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Degradation ratios across models for each attack}}{42}{table.caption.100}\protected@file@percent }
\newlabel{tab:attack_results_degradation}{{6.2}{42}{Degradation ratios across models for each attack}{table.caption.100}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Deviation scores across models for each attack}}{42}{table.caption.101}\protected@file@percent }
\newlabel{tab:attack_results_deviation}{{6.3}{42}{Deviation scores across models for each attack}{table.caption.101}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Lipschitz Estimate (local sensitivity) across models for each attack}}{42}{table.caption.102}\protected@file@percent }
\newlabel{tab:attack_results_sensitivity}{{6.4}{42}{Lipschitz Estimate (local sensitivity) across models for each attack}{table.caption.102}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Visual Analysis}{42}{section.6.3}\protected@file@percent }
\newlabel{fig:fgsm_spiral_LTC}{{\caption@xref {fig:fgsm_spiral_LTC}{ on input line 248}}{43}{FGSM Attack Results}{figure.caption.105}{}}
\newlabel{sub@fig:fgsm_spiral_LTC}{{}{43}{FGSM Attack Results}{figure.caption.105}{}}
\newlabel{fig:fgsm_spiral_TCN}{{\caption@xref {fig:fgsm_spiral_TCN}{ on input line 253}}{43}{FGSM Attack Results}{figure.caption.105}{}}
\newlabel{sub@fig:fgsm_spiral_TCN}{{}{43}{FGSM Attack Results}{figure.caption.105}{}}
\newlabel{fig:fgsm_spiral_lstm}{{\caption@xref {fig:fgsm_spiral_lstm}{ on input line 258}}{43}{FGSM Attack Results}{figure.caption.105}{}}
\newlabel{sub@fig:fgsm_spiral_lstm}{{}{43}{FGSM Attack Results}{figure.caption.105}{}}
\newlabel{fig:fgsm_spiral_transformer}{{\caption@xref {fig:fgsm_spiral_transformer}{ on input line 263}}{43}{FGSM Attack Results}{figure.caption.105}{}}
\newlabel{sub@fig:fgsm_spiral_transformer}{{}{43}{FGSM Attack Results}{figure.caption.105}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Predicted, Target, and Adversarial projections under FGSM attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}}{43}{figure.caption.105}\protected@file@percent }
\newlabel{fig:fsgm_spirals}{{6.2}{43}{Predicted, Target, and Adversarial projections under FGSM attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral}{figure.caption.105}{}}
\newlabel{fig:pgd_spiral_LTC}{{\caption@xref {fig:pgd_spiral_LTC}{ on input line 287}}{44}{PGD Attack Results}{figure.caption.107}{}}
\newlabel{sub@fig:pgd_spiral_LTC}{{}{44}{PGD Attack Results}{figure.caption.107}{}}
\newlabel{fig:pgd_spiral_TCN}{{\caption@xref {fig:pgd_spiral_TCN}{ on input line 292}}{44}{PGD Attack Results}{figure.caption.107}{}}
\newlabel{sub@fig:pgd_spiral_TCN}{{}{44}{PGD Attack Results}{figure.caption.107}{}}
\newlabel{fig:pgd_spiral_lstm}{{\caption@xref {fig:pgd_spiral_lstm}{ on input line 297}}{44}{PGD Attack Results}{figure.caption.107}{}}
\newlabel{sub@fig:pgd_spiral_lstm}{{}{44}{PGD Attack Results}{figure.caption.107}{}}
\newlabel{fig:pgd_spiral_transformer}{{\caption@xref {fig:pgd_spiral_transformer}{ on input line 302}}{44}{PGD Attack Results}{figure.caption.107}{}}
\newlabel{sub@fig:pgd_spiral_transformer}{{}{44}{PGD Attack Results}{figure.caption.107}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Predicted, Target, and Adversarial projections under PGD attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}}{44}{figure.caption.107}\protected@file@percent }
\newlabel{fig:pgd_spirals}{{6.3}{44}{Predicted, Target, and Adversarial projections under PGD attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral}{figure.caption.107}{}}
\newlabel{fig:deepfool_spiral_LTC}{{\caption@xref {fig:deepfool_spiral_LTC}{ on input line 324}}{45}{Deepfool-Like Attack Results}{figure.caption.109}{}}
\newlabel{sub@fig:deepfool_spiral_LTC}{{}{45}{Deepfool-Like Attack Results}{figure.caption.109}{}}
\newlabel{fig:deepfool_spiral_TCN}{{\caption@xref {fig:deepfool_spiral_TCN}{ on input line 329}}{45}{Deepfool-Like Attack Results}{figure.caption.109}{}}
\newlabel{sub@fig:deepfool_spiral_TCN}{{}{45}{Deepfool-Like Attack Results}{figure.caption.109}{}}
\newlabel{fig:deepfool_spiral_lstm}{{\caption@xref {fig:deepfool_spiral_lstm}{ on input line 334}}{45}{Deepfool-Like Attack Results}{figure.caption.109}{}}
\newlabel{sub@fig:deepfool_spiral_lstm}{{}{45}{Deepfool-Like Attack Results}{figure.caption.109}{}}
\newlabel{fig:deepfool_spiral_transformer}{{\caption@xref {fig:deepfool_spiral_transformer}{ on input line 339}}{45}{Deepfool-Like Attack Results}{figure.caption.109}{}}
\newlabel{sub@fig:deepfool_spiral_transformer}{{}{45}{Deepfool-Like Attack Results}{figure.caption.109}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Predicted, Target, and Adversarial projections under Deepfool-Like attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}}{45}{figure.caption.109}\protected@file@percent }
\newlabel{fig:deepfool_spirals}{{6.4}{45}{Predicted, Target, and Adversarial projections under Deepfool-Like attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral}{figure.caption.109}{}}
\newlabel{fig:spsa_spiral_LTC}{{\caption@xref {fig:spsa_spiral_LTC}{ on input line 359}}{46}{SPSA Results}{figure.caption.111}{}}
\newlabel{sub@fig:spsa_spiral_LTC}{{}{46}{SPSA Results}{figure.caption.111}{}}
\newlabel{fig:spsa_spiral_TCN}{{\caption@xref {fig:spsa_spiral_TCN}{ on input line 364}}{46}{SPSA Results}{figure.caption.111}{}}
\newlabel{sub@fig:spsa_spiral_TCN}{{}{46}{SPSA Results}{figure.caption.111}{}}
\newlabel{fig:spsa_spiral_lstm}{{\caption@xref {fig:spsa_spiral_lstm}{ on input line 369}}{46}{SPSA Results}{figure.caption.111}{}}
\newlabel{sub@fig:spsa_spiral_lstm}{{}{46}{SPSA Results}{figure.caption.111}{}}
\newlabel{fig:spsa_spiral_transformer}{{\caption@xref {fig:spsa_spiral_transformer}{ on input line 374}}{46}{SPSA Results}{figure.caption.111}{}}
\newlabel{sub@fig:spsa_spiral_transformer}{{}{46}{SPSA Results}{figure.caption.111}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Predicted, Target, and Adversarial projections under SPSA attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}}{46}{figure.caption.111}\protected@file@percent }
\newlabel{fig:spsa_spirals}{{6.5}{46}{Predicted, Target, and Adversarial projections under SPSA attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral}{figure.caption.111}{}}
\newlabel{fig:time_warping_spiral_LTC}{{\caption@xref {fig:time_warping_spiral_LTC}{ on input line 394}}{47}{Time Warping Attack Results}{figure.caption.113}{}}
\newlabel{sub@fig:time_warping_spiral_LTC}{{}{47}{Time Warping Attack Results}{figure.caption.113}{}}
\newlabel{fig:time_warping_spiral_TCN}{{\caption@xref {fig:time_warping_spiral_TCN}{ on input line 399}}{47}{Time Warping Attack Results}{figure.caption.113}{}}
\newlabel{sub@fig:time_warping_spiral_TCN}{{}{47}{Time Warping Attack Results}{figure.caption.113}{}}
\newlabel{fig:time_warping_spiral_lstm}{{\caption@xref {fig:time_warping_spiral_lstm}{ on input line 404}}{47}{Time Warping Attack Results}{figure.caption.113}{}}
\newlabel{sub@fig:time_warping_spiral_lstm}{{}{47}{Time Warping Attack Results}{figure.caption.113}{}}
\newlabel{fig:time_warping_spiral_transformer}{{\caption@xref {fig:time_warping_spiral_transformer}{ on input line 409}}{47}{Time Warping Attack Results}{figure.caption.113}{}}
\newlabel{sub@fig:time_warping_spiral_transformer}{{}{47}{Time Warping Attack Results}{figure.caption.113}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Predicted, Target, and Adversarial projections under Time Warping attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}}{47}{figure.caption.113}\protected@file@percent }
\newlabel{fig:time_warping_spirals}{{6.6}{47}{Predicted, Target, and Adversarial projections under Time Warping attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral}{figure.caption.113}{}}
\newlabel{fig:continuous_time_spiral_LTC}{{\caption@xref {fig:continuous_time_spiral_LTC}{ on input line 429}}{48}{Continuous-Time Adversarial Perturbation Attack Results}{figure.caption.115}{}}
\newlabel{sub@fig:continuous_time_spiral_LTC}{{}{48}{Continuous-Time Adversarial Perturbation Attack Results}{figure.caption.115}{}}
\newlabel{fig:continuous_time_spiral_TCN}{{\caption@xref {fig:continuous_time_spiral_TCN}{ on input line 434}}{48}{Continuous-Time Adversarial Perturbation Attack Results}{figure.caption.115}{}}
\newlabel{sub@fig:continuous_time_spiral_TCN}{{}{48}{Continuous-Time Adversarial Perturbation Attack Results}{figure.caption.115}{}}
\newlabel{fig:continuous_time_spiral_lstm}{{\caption@xref {fig:continuous_time_spiral_lstm}{ on input line 439}}{48}{Continuous-Time Adversarial Perturbation Attack Results}{figure.caption.115}{}}
\newlabel{sub@fig:continuous_time_spiral_lstm}{{}{48}{Continuous-Time Adversarial Perturbation Attack Results}{figure.caption.115}{}}
\newlabel{fig:continuous_time_spiral_transformer}{{\caption@xref {fig:continuous_time_spiral_transformer}{ on input line 444}}{48}{Continuous-Time Adversarial Perturbation Attack Results}{figure.caption.115}{}}
\newlabel{sub@fig:continuous_time_spiral_transformer}{{}{48}{Continuous-Time Adversarial Perturbation Attack Results}{figure.caption.115}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Predicted, Target, and Adversarial projections under Continuous-Time Adversarial Perturbation attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral.}}{48}{figure.caption.115}\protected@file@percent }
\newlabel{fig:continuous_time_spirals}{{6.7}{48}{Predicted, Target, and Adversarial projections under Continuous-Time Adversarial Perturbation attack on LTC, TCN, LSTM and Transformer models. Using the same denormalised input spiral}{figure.caption.115}{}}
\citation{madry2018towards}
\citation{vincent2008extracting}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Discussion of Results}{49}{section.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Defences for Liquid Neural Networks}{49}{section.6.5}\protected@file@percent }
\citation{dupont2019augmented}
\citation{finlay2020trainable}
\citation{wang2018adversarial}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces LNN membrane potential evolution with and without regularisation. Regularisation limits state drift, improving stability under input perturbations.}}{50}{figure.caption.122}\protected@file@percent }
\newlabel{fig:lnn_dynamics_defence}{{6.8}{50}{LNN membrane potential evolution with and without regularisation. Regularisation limits state drift, improving stability under input perturbations}{figure.caption.122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Illustration of low-pass temporal filtering as a defence mechanism. Sharp perturbations are smoothed before reaching the model.}}{50}{figure.caption.124}\protected@file@percent }
\newlabel{fig:temporal_filter}{{6.9}{50}{Illustration of low-pass temporal filtering as a defence mechanism. Sharp perturbations are smoothed before reaching the model}{figure.caption.124}{}}
\citation{zhang2022towards}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{52}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Summary of Work}{52}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Future Work}{52}{section.7.2}\protected@file@percent }
\bibstyle{vancouver}
\bibdata{bibs/fyp}
\bibcite{hasaniLiquidTimeconstantNetworks2021}{1}
\bibcite{frank2024learning}{2}
\bibcite{tedxtalksLiquidNeuralNetworks2023}{3}
\bibcite{chahineRobustFlightNavigation2023}{4}
\bibcite{szegedy2013intriguing}{5}
\bibcite{goodfellow2014explaining}{6}
\bibcite{liquidTimeConstant}{7}
\bibcite{madry2018towards}{8}
\bibcite{moosavi2016deepfool}{9}
\bibcite{uesato2018adversarial}{10}
\bibcite{cisse2017houdini}{11}
\bibcite{sun2018natural}{12}
\bibcite{weng2018evaluating}{13}
\bibcite{dong2020benchmarking}{14}
\bibcite{henriksenEfficientNeuralNetwork}{15}
\bibcite{zhangEfficientNeuralNetwork2018}{16}
\bibcite{ltctutorial2022}{17}
\bibcite{vaswani2017attention}{18}
\bibcite{goodfellow2015explaining}{19}
\bibcite{autoLiRPA2022}{20}
\bibcite{vincent2008extracting}{21}
\bibcite{dupont2019augmented}{22}
\bibcite{finlay2020trainable}{23}
\bibcite{wang2018adversarial}{24}
\bibcite{zhang2022towards}{25}
\gdef \@abspage@last{56}
