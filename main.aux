\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{chahineRobustFlightNavigation2023}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{6}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{hasaniLiquidTimeconstantNetworks2021}
\citation{tedxtalksLiquidNeuralNetworks2023}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background and Literature Review}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Liquid Neural Networks}{7}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Continuous-Time Dynamical Systems/Differential Equations}{7}{subsection.2.1.1}\protected@file@percent }
\newlabel{eq:1}{{2.1}{8}{Continuous-Time Dynamical Systems/Differential Equations}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}LNN Training}{8}{subsection.2.1.2}\protected@file@percent }
\citation{chahineRobustFlightNavigation2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}LNN Inference}{9}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Advantages of LNNs}{9}{subsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.a}Temporal Modelling}{9}{subsubsection.2.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.b}Adaptability}{9}{subsubsection.2.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.c}Efficiency}{9}{subsubsection.2.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.d}Stability}{10}{subsubsection.2.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Neural Network Verification}{10}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Verification Problem}{10}{subsection.2.2.1}\protected@file@percent }
\citation{henriksenEfficientNeuralNetwork}
\citation{WhatRecurrentNeural2021}
\newlabel{verification_def}{{2.2.1}{11}{The Verification Problem}{subsection.2.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Motivation}{11}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Recurrent Neural Networks}{11}{subsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Feedforward (traditional) vs. Recurrent Neural Networks}}{11}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example_image}{{2.1}{11}{Feedforward (traditional) vs. Recurrent Neural Networks}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Robustness Verification for Recurrent Neural Networks}{12}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Symbolic and Interval Propagation Methods}{12}{subsection.2.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.a}SIP (Symbolic Interval Propagation)}{12}{subsubsection.2.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Steps in SIP:}{12}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages for Liquid Neural Networks}{13}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Considerations}{13}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.b}CROWN (Certified Robustness to Weight Perturbations)}{13}{subsubsection.2.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mathematical Framework}{13}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bounding Nonlinearities}{13}{section*.9}\protected@file@percent }
\citation{zhangEfficientNeuralNetwork2018}
\@writefile{toc}{\contentsline {paragraph}{Output Certification}{14}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Application to Neural ODEs}{14}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Implementation}{14}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.c}Lipschitz-Based Methods}{14}{subsubsection.2.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mathematical Foundation}{14}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Estimation of the Lipschitz Constant}{14}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Verification Applications}{15}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Implementation}{15}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Liquid Neural Network Design and Implementation}{16}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Design Overview}{16}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Wiring and Connectivity}{16}{section.3.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}{\ignorespaces Simplified RandomWiring class}}{17}{lstlisting.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}LTC Neuron Dynamics}{17}{section.3.3}\protected@file@percent }
\newlabel{eq:v_update}{{3.1}{17}{LTC Neuron Dynamics}{equation.3.3.1}{}}
\newlabel{eq:v_update_denominator}{{3.2}{17}{LTC Neuron Dynamics}{equation.3.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Illustrative internal membrane voltage trace across $K$ ODE unfolding steps for a single neuron. This entire graph represents the internal dynamics used to compute $v_i^{(t+1)}$ from $v_i^{(t)}$, in a single timestep.}}{18}{figure.caption.17}\protected@file@percent }
\newlabel{fig:lif_voltage_evolution}{{3.1}{18}{Illustrative internal membrane voltage trace across $K$ ODE unfolding steps for a single neuron. This entire graph represents the internal dynamics used to compute $v_i^{(t+1)}$ from $v_i^{(t)}$, in a single timestep}{figure.caption.17}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.2}{\ignorespaces Simplified LTC neuron forward method}}{19}{lstlisting.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Internal structure of a single LIF neuron used in the Liquid Time-Constant Network. Inputs undergo non-linear transformations based on trainable $\mu $ and $\sigma $, and the resulting activations are integrated using biophysical parameters (leak conductance $g_\text  {leak}$, membrane capacitance $C_m$, and reversal potentials $E_\text  {rev}$).}}{19}{figure.caption.18}\protected@file@percent }
\newlabel{fig:lif_neuron_detailed}{{3.2}{19}{Internal structure of a single LIF neuron used in the Liquid Time-Constant Network. Inputs undergo non-linear transformations based on trainable $\mu $ and $\sigma $, and the resulting activations are integrated using biophysical parameters (leak conductance $g_\text {leak}$, membrane capacitance $C_m$, and reversal potentials $E_\text {rev}$)}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Network Architecture}{19}{section.3.4}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.3}{\ignorespaces Structure of the LTCRNN module}}{20}{lstlisting.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Architecture of the Liquid Time-Constant Network (LNN). Inputs project through learned sensory filters to a sparsely recurrent Liquid Layer of LIF neurons. Dynamics are integrated using an internal ODE solver with unfolding. Final outputs are read from a low-dimensional projection.}}{20}{figure.caption.20}\protected@file@percent }
\newlabel{fig:lnn_architecture}{{3.3}{20}{Architecture of the Liquid Time-Constant Network (LNN). Inputs project through learned sensory filters to a sparsely recurrent Liquid Layer of LIF neurons. Dynamics are integrated using an internal ODE solver with unfolding. Final outputs are read from a low-dimensional projection}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Training Configuration (and dataset)}{20}{section.3.5}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.4}{\ignorespaces Simplified training loop for the LNN}}{21}{lstlisting.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Training Behaviour}{21}{section.3.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Training and validation loss over epochs.}}{22}{figure.caption.22}\protected@file@percent }
\newlabel{fig:lnn_loss}{{3.4}{22}{Training and validation loss over epochs}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Training and validation loss over epochs (zoomed).}}{22}{figure.caption.23}\protected@file@percent }
\newlabel{fig:lnn_loss_zoomed}{{3.5}{22}{Training and validation loss over epochs (zoomed)}{figure.caption.23}{}}
\newlabel{fig:lnn_training_validation_spiral_epoch_1}{{3.6a}{23}{Epoch 1}{figure.caption.25}{}}
\newlabel{sub@fig:lnn_training_validation_spiral_epoch_1}{{a}{23}{Epoch 1}{figure.caption.25}{}}
\newlabel{fig:lnn_training_validation_spiral_epoch_400}{{3.6b}{23}{Epoch 400}{figure.caption.25}{}}
\newlabel{sub@fig:lnn_training_validation_spiral_epoch_400}{{b}{23}{Epoch 400}{figure.caption.25}{}}
\newlabel{fig:lnn_training_validation_spiral_epoch_2000}{{3.6c}{23}{Epoch 2000}{figure.caption.25}{}}
\newlabel{sub@fig:lnn_training_validation_spiral_epoch_2000}{{c}{23}{Epoch 2000}{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces LLN predicted vs true spiral trajectories across training: early, mid, and final epochs (denormalised training and validation spiral)}}{23}{figure.caption.25}\protected@file@percent }
\newlabel{fig:lnn_spiral_progression_grid}{{3.6}{23}{LLN predicted vs true spiral trajectories across training: early, mid, and final epochs (denormalised training and validation spiral)}{figure.caption.25}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Comparative Models}{24}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction to Baseline Models}{24}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Temporal Convolutional Network (TCN)}{24}{section.4.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.1}{\ignorespaces Simplified TCN architecture}}{25}{lstlisting.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Temporal Convolutional Network (TCN) architecture}}{25}{figure.caption.30}\protected@file@percent }
\newlabel{fig:tcn_architecture}{{4.1}{25}{Temporal Convolutional Network (TCN) architecture}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Structure of a TCN Residual Block. Each convolution uses dilation to increase receptive field, while residual skip connections and dropout stabilise training.}}{26}{figure.caption.31}\protected@file@percent }
\newlabel{fig:tcn_residual_block}{{4.2}{26}{Structure of a TCN Residual Block. Each convolution uses dilation to increase receptive field, while residual skip connections and dropout stabilise training}{figure.caption.31}{}}
\newlabel{fig:tcn_training_validation_spiral_epoch_1}{{4.3a}{26}{Epoch 1}{figure.caption.34}{}}
\newlabel{sub@fig:tcn_training_validation_spiral_epoch_1}{{a}{26}{Epoch 1}{figure.caption.34}{}}
\newlabel{fig:tcn_training_validation_spiral_epoch_400}{{4.3b}{26}{Epoch 400 (early stopping point)}{figure.caption.34}{}}
\newlabel{sub@fig:tcn_training_validation_spiral_epoch_400}{{b}{26}{Epoch 400 (early stopping point)}{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces TCN predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}}{26}{figure.caption.34}\protected@file@percent }
\newlabel{fig:tcn_spiral_progression_grid}{{4.3}{26}{TCN predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}{figure.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Long Short-Term Memory Network (LSTM)}{27}{section.4.3}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.2}{\ignorespaces Simplified LSTM model structure}}{27}{lstlisting.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Architecture of the LSTM model. The left shows residual-enhanced flow through a 2-layer LSTM, while the right shows the LSTM unrolled across time with hidden and cell state transitions.}}{28}{figure.caption.39}\protected@file@percent }
\newlabel{fig:lstm_architecture_final}{{4.4}{28}{Architecture of the LSTM model. The left shows residual-enhanced flow through a 2-layer LSTM, while the right shows the LSTM unrolled across time with hidden and cell state transitions}{figure.caption.39}{}}
\newlabel{fig:lstm_training_validation_spiral_epoch_1}{{4.5a}{28}{Epoch 1}{figure.caption.42}{}}
\newlabel{sub@fig:lstm_training_validation_spiral_epoch_1}{{a}{28}{Epoch 1}{figure.caption.42}{}}
\newlabel{fig:lstm_training_validation_spiral_epoch_1000}{{4.5b}{28}{Epoch 1000 (early stopping point)}{figure.caption.42}{}}
\newlabel{sub@fig:lstm_training_validation_spiral_epoch_1000}{{b}{28}{Epoch 1000 (early stopping point)}{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces LSTM predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}}{28}{figure.caption.42}\protected@file@percent }
\newlabel{fig:lstm_spiral_progression_grid}{{4.5}{28}{LSTM predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}{figure.caption.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Transformer Model}{29}{section.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Architecture of the Transformer encoder used}}{30}{figure.caption.46}\protected@file@percent }
\newlabel{fig:transformer_encoder_diagram}{{4.6}{30}{Architecture of the Transformer encoder used}{figure.caption.46}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.3}{\ignorespaces Simplified Transformer architecture}}{30}{lstlisting.4.3}\protected@file@percent }
\newlabel{fig:transformer_training_validation_spiral_epoch_1}{{4.7a}{31}{Epoch 1}{figure.caption.50}{}}
\newlabel{sub@fig:transformer_training_validation_spiral_epoch_1}{{a}{31}{Epoch 1}{figure.caption.50}{}}
\newlabel{fig:transformer_training_validation_spiral_epoch_600}{{4.7b}{31}{Epoch 600 (early stopping point)}{figure.caption.50}{}}
\newlabel{sub@fig:transformer_training_validation_spiral_epoch_600}{{b}{31}{Epoch 600 (early stopping point)}{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Transformer predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}}{31}{figure.caption.50}\protected@file@percent }
\newlabel{fig:transformer_spiral_progression_grid}{{4.7}{31}{Transformer predicted vs true spiral trajectories across training: early and final epochs (denormalised training and validation spiral)}{figure.caption.50}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Adversarial Attack Methodology}{32}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction to Adversarial Attacks}{32}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Fast Gradient Sign Method (FGSM)}{32}{section.5.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}{\ignorespaces FGSM adversarial attack implementation}}{33}{lstlisting.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Projected Gradient Descent (PGD)}{33}{section.5.3}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.2}{\ignorespaces PGD Attack Loop (Simplified)}}{33}{lstlisting.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}DeepFool-Inspired Directional Attack}{34}{section.5.4}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.3}{\ignorespaces Directional (DeepFool-like) Gradient Attack}}{34}{lstlisting.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Simultaneous Perturbation Stochastic Approximation (SPSA)}{34}{section.5.5}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.4}{\ignorespaces Simplified SPSA implementation}}{35}{lstlisting.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Time-Warping Attack}{35}{section.5.6}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.5}{\ignorespaces Example Time-Warping Attack Function}}{36}{lstlisting.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Continuous-Time Perturbation Attack}{36}{section.5.7}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.6}{\ignorespaces Continuous-Time Perturbation Injection}}{37}{lstlisting.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Summary of Attack Design and Implementation Decisions}{37}{section.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.1}Attack Categories and Coverage}{37}{subsection.5.8.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Overview of attack types and model sensitivities.}}{37}{table.caption.71}\protected@file@percent }
\newlabel{tab:attack_summary}{{5.1}{37}{Overview of attack types and model sensitivities}{table.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.2}Implementation Consistency}{37}{subsection.5.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.3}Design Considerations}{38}{subsection.5.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bound Certification (Auto Lirpa)}{39}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Evaluation}{40}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:evaluation}{{7}{40}{Evaluation}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Quantitative Evaluation Metrics and Comparison}{40}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Aggregate Results}{41}{section.7.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Average degradation and deviation metrics across all attack types.}}{41}{table.caption.76}\protected@file@percent }
\newlabel{tab:agg_metrics}{{7.1}{41}{Average degradation and deviation metrics across all attack types}{table.caption.76}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Attack-Specific Breakdowns}{41}{section.7.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Degradation ratios across models for each attack. Lower is better.}}{41}{table.caption.77}\protected@file@percent }
\newlabel{tab:attack_results_degradation}{{7.2}{41}{Degradation ratios across models for each attack. Lower is better}{table.caption.77}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces ?}}{41}{table.caption.78}\protected@file@percent }
\newlabel{tab:attack_results_deviation}{{7.3}{41}{?}{table.caption.78}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.4}{\ignorespaces ?}}{42}{table.caption.79}\protected@file@percent }
\newlabel{tab:attack_results_sensitivity}{{7.4}{42}{?}{table.caption.79}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Qualitative Evaluation and Visual Analysis}{43}{section.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Comparative Discussion of Model Robustness}{44}{section.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Summary of Behaviour Under Attack}{44}{subsection.7.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Defences and Mitigation Strategies}{46}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction}{46}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Adversarial Training and Noise Injection}{46}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Gradient-Based Adversarial Training}{46}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Noise Injection During Training}{47}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Benefits and Trade-offs}{47}{subsection.8.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}Considerations}{47}{subsection.8.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Architectural Enhancements for Robustness}{47}{section.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Memory Mechanisms and Temporal Smoothing}{47}{subsection.8.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Receptive Field and Feature Redundancy}{47}{subsection.8.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Stability in Continuous-Time Models}{48}{subsection.8.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.4}Summary}{48}{subsection.8.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Input Preprocessing and Temporal Defences}{48}{section.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Low-Pass Filtering}{48}{subsection.8.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}Interpolation and Resampling Defences}{48}{subsection.8.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.3}Temporal Quantisation}{48}{subsection.8.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.4}Trade-offs and Limitations}{49}{subsection.8.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.5}Summary}{49}{subsection.8.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Model-Specific Mitigation Insights}{49}{section.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}LSTM: Sequential Memory Vulnerabilities}{49}{subsection.8.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}TCN: Localised Perturbation Sensitivity}{49}{subsection.8.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}LNN: ODE Sensitivity and Stability Management}{49}{subsection.8.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.4}Summary Table}{50}{subsection.8.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Summary of recommended mitigation strategies by model.}}{50}{table.caption.98}\protected@file@percent }
\newlabel{tab:model_defences}{{8.1}{50}{Summary of recommended mitigation strategies by model}{table.caption.98}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}Limitations and Future Work}{50}{section.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.1}Limitations}{50}{subsection.8.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.2}Future Work}{50}{subsection.8.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.3}Conclusion}{51}{subsection.8.6.3}\protected@file@percent }
\bibstyle{vancouver}
\bibdata{bibs/fyp}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Conclusion}{52}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{chahineRobustFlightNavigation2023}{1}
\bibcite{hasaniLiquidTimeconstantNetworks2021}{2}
\bibcite{tedxtalksLiquidNeuralNetworks2023}{3}
\bibcite{henriksenEfficientNeuralNetwork}{4}
\bibcite{WhatRecurrentNeural2021}{5}
\bibcite{zhangEfficientNeuralNetwork2018}{6}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Declaration}{54}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\gdef \@abspage@last{54}
